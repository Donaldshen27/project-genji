{
  "ticket": "P3C4-001-008",
  "description": "Fix test assertions for model persistence to correctly verify model_hash for model artifacts and data_hash for OOF artifacts. The persistence module was already correctly implemented, but tests had incorrect hash field references.",
  "context": "Per ticket P3C4-001-008 in tickets/work_items.json, the persistence module provides save/load functions for trained models (using joblib with model_hash) and OOF predictions (using parquet with data_hash). The existing persistence.py implementation is correct, but test_model2_persistence.py had three incorrect assertions comparing the wrong hash fields.",
  "notes": "ModelArtifactMetadata uses 'model_hash' field (line 53 of persistence.py), OOFArtifactMetadata uses 'data_hash' field (line 90). Tests at lines 56, 76 incorrectly used data_hash for models (should be model_hash), and line 127 incorrectly used model_hash for OOF (should be data_hash). All edge cases from the ticket are already covered: directory creation (lines 79-86, 117-123), overwrite warnings (lines 79-92, 141-149), missing files (lines 106-109), corrupt models (hash mismatch at lines 152-166), and empty DataFrames (lines 169-180).",
  "files": [
    {
      "path": "src/model2/persistence.py",
      "content": "\"\"\"Persistence utilities for Model 2 base models and OOF predictions.\n\nProvides helpers to store and restore trained :class:`BaseModelTrainer`\ninstances (Ridge, XGBoost) and their out-of-fold (OOF) predictions with\nrobust error handling. Ensures round-trip integrity by computing stable\nhashes of serialized artifacts and embedding them into the saved files.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport contextlib\nimport json\nimport logging\nimport os\nimport tempfile\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timezone\nfrom hashlib import sha256\nfrom pathlib import Path\nfrom typing import Any, Generic, TypeVar\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom pandas.util import hash_pandas_object\n\nfrom .base_models import BaseModelTrainer\n\nlogger = logging.getLogger(__name__)\n\n# Constants -----------------------------------------------------------------\n\nMODEL_ARTIFACT_VERSION = \"1.0\"\nOOF_ARTIFACT_VERSION = \"1.0\"\nOOF_METADATA_KEY = b\"model2_oof_metadata\"\n\nT = TypeVar(\"T\", bound=BaseModelTrainer)\n\n\n# Dataclasses ---------------------------------------------------------------\n\n@dataclass(frozen=True)\nclass ModelArtifactMetadata(Generic[T]):\n    \"\"\"Metadata stored alongside a serialized model artifact.\"\"\"\n\n    artifact_version: str\n    class_path: str\n    created_at_utc: str\n    library_versions: dict[str, str]\n    model_params: dict[str, Any] | None\n    model_hash: str\n    extra: dict[str, Any] = field(default_factory=dict)\n\n    def to_dict(self) -> dict[str, Any]:\n        return {\n            \"artifact_version\": self.artifact_version,\n            \"class_path\": self.class_path,\n            \"created_at_utc\": self.created_at_utc,\n            \"library_versions\": self.library_versions,\n            \"model_params\": self.model_params,\n            \"model_hash\": self.model_hash,\n            \"extra\": self.extra,\n        }\n\n    @classmethod\n    def from_dict(cls, data: dict[str, Any]) -> \"ModelArtifactMetadata\":\n        default_versions: dict[str, str] = {}\n        return cls(\n            artifact_version=data.get(\"artifact_version\", MODEL_ARTIFACT_VERSION),\n            class_path=data.get(\"class_path\", \"\"),\n            created_at_utc=data.get(\"created_at_utc\", \"\"),\n            library_versions=data.get(\"library_versions\", default_versions),\n            model_params=data.get(\"model_params\"),\n            model_hash=data.get(\"model_hash\", \"\"),\n            extra=data.get(\"extra\", {}),\n        )\n\n\n@dataclass(frozen=True)\nclass OOFArtifactMetadata:\n    \"\"\"Metadata embedded into the OOF parquet file.\"\"\"\n\n    artifact_version: str\n    created_at_utc: str\n    num_rows: int\n    index_names: tuple[str | None, ...]\n    column_dtypes: dict[str, str]\n    data_hash: str\n\n    def to_json_bytes(self) -> bytes:\n        payload = {\n            \"artifact_version\": self.artifact_version,\n            \"created_at_utc\": self.created_at_utc,\n            \"num_rows\": self.num_rows,\n            \"index_names\": list(self.index_names),\n            \"column_dtypes\": self.column_dtypes,\n            \"data_hash\": self.data_hash,\n        }\n        return json.dumps(payload, sort_keys=True).encode(\"utf-8\")\n\n    @classmethod\n    def from_json_bytes(cls, payload: bytes) -> \"OOFArtifactMetadata\":\n        data = json.loads(payload.decode(\"utf-8\"))\n        return cls(\n            artifact_version=data.get(\"artifact_version\", OOF_ARTIFACT_VERSION),\n            created_at_utc=data.get(\"created_at_utc\", \"\"),\n            num_rows=int(data.get(\"num_rows\", 0)),\n            index_names=tuple(data.get(\"index_names\", [])),\n            column_dtypes=data.get(\"column_dtypes\", {}),\n            data_hash=data.get(\"data_hash\", \"\"),\n        )\n\n\n# Helpers -------------------------------------------------------------------\n\ndef _ensure_parent_dir(path: Path) -> None:\n    try:\n        path.parent.mkdir(parents=True, exist_ok=True)\n    except Exception as exc:  # pragma: no cover - propagate with context\n        raise OSError(f\"Failed to create parent directory for {path}: {exc}\") from exc\n\n\ndef _warn_on_overwrite(path: Path) -> None:\n    if path.exists():\n        logger.warning(\"Overwriting existing artifact at %s\", path)\n\n\ndef _atomic_joblib_dump(obj: Any, path: Path, *, compress: int = 3) -> None:\n    tmp_file = tempfile.NamedTemporaryFile(\n        dir=str(path.parent), prefix=path.stem, suffix=\".tmp\", delete=False\n    )\n    tmp_path = Path(tmp_file.name)\n    tmp_file.close()\n    try:\n        joblib.dump(obj, tmp_path, compress=compress)\n        os.replace(tmp_path, path)\n    except Exception as exc:  # pragma: no cover - re-raise with cleanup\n        with contextlib.suppress(FileNotFoundError):\n            tmp_path.unlink()\n        raise OSError(f\"Failed to write joblib artifact to {path}: {exc}\") from exc\n\n\ndef _atomic_parquet_write(table: pa.Table, path: Path) -> None:\n    tmp_file = tempfile.NamedTemporaryFile(\n        dir=str(path.parent), prefix=path.stem, suffix=\".tmp\", delete=False\n    )\n    tmp_path = Path(tmp_file.name)\n    tmp_file.close()\n    try:\n        pq.write_table(table, tmp_path)\n        os.replace(tmp_path, path)\n    except Exception as exc:  # pragma: no cover - re-raise with cleanup\n        with contextlib.suppress(FileNotFoundError):\n            tmp_path.unlink()\n        raise OSError(f\"Failed to write parquet artifact to {path}: {exc}\") from exc\n\n\ndef _current_utc_timestamp() -> str:\n    return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\n\ndef _model_class_path(model: BaseModelTrainer) -> str:\n    return f\"{model.__class__.__module__}.{model.__class__.__name__}\"\n\n\ndef _collect_library_versions() -> dict[str, str]:\n    return {\n        \"joblib\": getattr(joblib, \"__version__\", \"unknown\"),\n        \"numpy\": getattr(np, \"__version__\", \"unknown\"),\n        \"pandas\": getattr(pd, \"__version__\", \"unknown\"),\n        \"pyarrow\": getattr(pa, \"__version__\", \"unknown\"),\n    }\n\n\ndef _build_model_metadata(model: BaseModelTrainer) -> ModelArtifactMetadata:\n    metadata = ModelArtifactMetadata(\n        artifact_version=MODEL_ARTIFACT_VERSION,\n        class_path=_model_class_path(model),\n        created_at_utc=_current_utc_timestamp(),\n        library_versions=_collect_library_versions(),\n        model_params=model.get_params(),\n        model_hash=joblib.hash(model),\n        extra={},\n    )\n    return metadata\n\n\ndef _validate_model_is_fitted(model: BaseModelTrainer) -> None:\n    if hasattr(model, \"_is_fitted\") and not getattr(model, \"_is_fitted\"):\n        raise ValueError(\"Cannot persist an unfitted model trainer.\")\n\n\ndef _prepare_oof_frame(oof_df: pd.DataFrame, *, copy_frame: bool = True) -> pd.DataFrame:\n    if not isinstance(oof_df, pd.DataFrame):\n        raise TypeError(\"oof_df must be a pandas DataFrame.\")\n\n    required_columns = {\"prediction\", \"fold_id\"}\n    missing = required_columns - set(oof_df.columns)\n    if missing:\n        raise ValueError(\n            f\"OOF predictions missing required columns: {sorted(missing)}. \"\n            f\"Expected columns: {sorted(required_columns)}\"\n        )\n\n    frame = oof_df.copy(deep=True) if copy_frame else oof_df\n\n    # Enforce dtypes for deterministic serialization\n    try:\n        frame[\"prediction\"] = frame[\"prediction\"].astype(np.float32)\n    except (TypeError, ValueError) as exc:\n        raise ValueError(\"OOF 'prediction' column must be numeric.\") from exc\n\n    try:\n        frame[\"fold_id\"] = frame[\"fold_id\"].astype(np.int8)\n    except (TypeError, ValueError) as exc:\n        raise ValueError(\"OOF 'fold_id' column must be integer coercible.\") from exc\n\n    if not np.isfinite(frame[\"prediction\"].to_numpy()).all():\n        raise ValueError(\"OOF predictions contain NaN or infinite values.\")\n\n    if isinstance(frame.index, pd.MultiIndex):\n        index_names = tuple(frame.index.names)\n        if any(name is None for name in index_names):\n            logger.warning(\n                \"OOF MultiIndex has unnamed levels; consider setting explicit names for clarity.\"\n            )\n    else:\n        logger.warning(\n            \"OOF predictions do not use a MultiIndex; downstream joins may rely on MultiIndex indices.\"\n        )\n\n    return frame\n\n\ndef _fingerprint_dataframe(df: pd.DataFrame) -> str:\n    hashed = hash_pandas_object(df, index=True, categorize=True)\n    return sha256(hashed.values.tobytes()).hexdigest()\n\n\ndef _build_oof_metadata(oof_df: pd.DataFrame) -> OOFArtifactMetadata:\n    if isinstance(oof_df.index, pd.MultiIndex):\n        index_names = tuple(oof_df.index.names)\n    else:\n        index_names = (oof_df.index.name,)\n    metadata = OOFArtifactMetadata(\n        artifact_version=OOF_ARTIFACT_VERSION,\n        created_at_utc=_current_utc_timestamp(),\n        num_rows=int(len(oof_df)),\n        index_names=index_names,\n        column_dtypes={col: str(oof_df[col].dtype) for col in oof_df.columns},\n        data_hash=_fingerprint_dataframe(oof_df),\n    )\n    return metadata\n\n\ndef _prepare_oof_table(oof_df: pd.DataFrame) -> pa.Table:\n    try:\n        table = pa.Table.from_pandas(oof_df, preserve_index=True)\n    except Exception as exc:  # pragma: no cover - propagate with context\n        raise ValueError(f\"Failed to convert OOF predictions to pyarrow Table: {exc}\") from exc\n    return table\n\n\n# Public API ----------------------------------------------------------------\n\ndef save_trained_model(\n    model: BaseModelTrainer,\n    output_path: str | Path,\n    *,\n    compress: int = 3,\n    extra_metadata: dict[str, Any] | None = None,\n) -> ModelArtifactMetadata:\n    \"\"\"\n    Persist a fitted :class:`BaseModelTrainer` instance to disk via joblib.\n\n    Args:\n        model: Trained model trainer (RidgeTrainer, XGBoostTrainer, etc.).\n        output_path: Destination ``.pkl`` file path.\n        compress: joblib compression level (default 3).\n        extra_metadata: Optional dictionary merged into metadata[\"extra\"].\n\n    Returns:\n        ``ModelArtifactMetadata`` describing the saved artifact.\n\n    Raises:\n        ValueError: If the model is unfitted or path suffix is incorrect.\n        OSError: If disk operations fail (permissions, disk full, etc.).\n    \"\"\"\n    output = Path(output_path)\n    if output.suffix != \".pkl\":\n        raise ValueError(f\"Model artifact path must end with '.pkl' (got {output}).\")\n\n    _validate_model_is_fitted(model)\n    _ensure_parent_dir(output)\n    _warn_on_overwrite(output)\n\n    metadata = _build_model_metadata(model)\n    if extra_metadata:\n        metadata.extra.update(extra_metadata)\n\n    artifact_payload = {\n        \"model\": model,\n        \"metadata\": metadata.to_dict(),\n    }\n\n    _atomic_joblib_dump(artifact_payload, output, compress=compress)\n\n    logger.info(\n        \"Saved model artifact for %s to %s (hash=%s)\",\n        metadata.class_path,\n        output,\n        metadata.model_hash,\n    )\n    return metadata\n\n\ndef load_trained_model(\n    input_path: str | Path,\n    expected_type: type[T] | None = None,\n    *,\n    verify_hash: bool = True,\n) -> tuple[T, ModelArtifactMetadata]:\n    \"\"\"\n    Load a persisted :class:`BaseModelTrainer` instance from disk.\n\n    Args:\n        input_path: Path to the ``.pkl`` artifact.\n        expected_type: Optional subclass of :class:`BaseModelTrainer` to enforce.\n        verify_hash: Whether to compare stored hash with recomputed value.\n\n    Returns:\n        Tuple of ``(model, metadata)``.\n\n    Raises:\n        FileNotFoundError: If the artifact path does not exist.\n        OSError: If joblib fails to deserialize the payload.\n        TypeError: If the loaded object is not the expected trainer type.\n        ValueError: Hash mismatch indicating potential corruption.\n    \"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Model artifact not found at {input_path}.\")\n\n    try:\n        payload = joblib.load(input_path)\n    except Exception as exc:  # pragma: no cover - propagate with context\n        raise OSError(f\"Failed to load model artifact from {input_path}: {exc}\") from exc\n\n    if isinstance(payload, dict) and \"model\" in payload:\n        model = payload[\"model\"]\n        metadata_dict = payload.get(\"metadata\", {})\n        metadata = ModelArtifactMetadata.from_dict(metadata_dict)\n    else:\n        model = payload\n        metadata = _build_model_metadata(model)\n        logger.warning(\n            \"Model artifact at %s is missing metadata; generated metadata on load.\", input_path\n        )\n\n    if not isinstance(model, BaseModelTrainer):\n        raise TypeError(\n            f\"Loaded object from {input_path} is not a BaseModelTrainer (got {type(model)}).\"\n        )\n\n    if expected_type and not isinstance(model, expected_type):\n        raise TypeError(\n            f\"Expected model type {expected_type.__name__}, \"\n            f\"but loaded {type(model).__name__} from {input_path}.\"\n        )\n\n    if verify_hash:\n        current_hash = joblib.hash(model)\n        if metadata.model_hash and metadata.model_hash != current_hash:\n            raise ValueError(\n                f\"Model hash mismatch for {input_path}. \"\n                f\"Expected {metadata.model_hash}, observed {current_hash}.\"\n            )\n\n    logger.info(\n        \"Loaded model artifact from %s (class=%s, hash=%s)\",\n        input_path,\n        metadata.class_path,\n        metadata.model_hash or \"<missing>\",\n    )\n    return model, metadata\n\n\ndef save_oof_predictions(\n    oof_df: pd.DataFrame,\n    output_path: str | Path,\n) -> OOFArtifactMetadata:\n    \"\"\"\n    Persist out-of-fold predictions to a parquet file with embedded metadata.\n\n    Args:\n        oof_df: OOF predictions DataFrame (expects ``prediction`` + ``fold_id`` columns).\n        output_path: Destination ``.parquet`` file path.\n\n    Returns:\n        ``OOFArtifactMetadata`` describing the saved parquet artifact.\n\n    Raises:\n        ValueError: If the DataFrame schema is invalid.\n        OSError: If parquet write fails (permissions, disk full, etc.).\n    \"\"\"\n    output = Path(output_path)\n    if output.suffix != \".parquet\":\n        raise ValueError(f\"OOF artifact path must end with '.parquet' (got {output}).\")\n\n    prepared = _prepare_oof_frame(oof_df, copy_frame=True)\n    metadata = _build_oof_metadata(prepared)\n\n    table = _prepare_oof_table(prepared)\n    existing_metadata = dict(table.schema.metadata or {})\n    existing_metadata[OOF_METADATA_KEY] = metadata.to_json_bytes()\n    table = table.replace_schema_metadata(existing_metadata)\n\n    _ensure_parent_dir(output)\n    _warn_on_overwrite(output)\n    _atomic_parquet_write(table, output)\n\n    logger.info(\n        \"Saved OOF predictions to %s (rows=%s, hash=%s)\",\n        output,\n        metadata.num_rows,\n        metadata.data_hash,\n    )\n    return metadata\n\n\ndef load_oof_predictions(\n    input_path: str | Path,\n    *,\n    verify_hash: bool = True,\n) -> tuple[pd.DataFrame, OOFArtifactMetadata]:\n    \"\"\"\n    Load OOF predictions from parquet, optionally verifying integrity hash.\n\n    Args:\n        input_path: Path to parquet artifact.\n        verify_hash: Whether to validate hash stored in metadata.\n\n    Returns:\n        Tuple of ``(DataFrame, metadata)``.\n\n    Raises:\n        FileNotFoundError: If path does not exist.\n        OSError: If parquet read fails.\n        ValueError: If hash validation fails or schema invalid.\n    \"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"OOF predictions parquet not found at {input_path}.\")\n\n    try:\n        table = pq.read_table(input_path)\n    except Exception as exc:  # pragma: no cover - propagate with context\n        raise OSError(f\"Failed to read OOF parquet from {input_path}: {exc}\") from exc\n\n    raw_metadata = table.schema.metadata or {}\n    metadata_bytes = raw_metadata.get(OOF_METADATA_KEY)\n    if metadata_bytes is None:\n        logger.warning(\"OOF parquet %s missing embedded metadata; recreating metadata.\", input_path)\n        dataframe = table.to_pandas()\n        prepared = _prepare_oof_frame(dataframe, copy_frame=True)\n        metadata = _build_oof_metadata(prepared)\n        if verify_hash:\n            # Hash verified implicitly by building from prepared frame\n            pass\n        return prepared, metadata\n\n    metadata = OOFArtifactMetadata.from_json_bytes(metadata_bytes)\n\n    dataframe = table.to_pandas()\n    prepared = _prepare_oof_frame(dataframe, copy_frame=True)\n\n    if verify_hash and metadata.data_hash:\n        current_hash = _fingerprint_dataframe(prepared)\n        if metadata.data_hash != current_hash:\n            raise ValueError(\n                f\"OOF predictions hash mismatch for {input_path}. \"\n                f\"Expected {metadata.data_hash}, observed {current_hash}.\"\n            )\n\n    logger.info(\n        \"Loaded OOF predictions from %s (rows=%s, hash=%s)\",\n        input_path,\n        metadata.num_rows,\n        metadata.data_hash or \"<missing>\",\n    )\n    return prepared, metadata\n"
    }
  ],
  "tests": [
    {
      "path": "tests/unit/test_model2_persistence.py",
      "content": "\"\"\"Unit tests for Model 2 persistence utilities.\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport pytest\n\nfrom src.model2.base_models import RidgeTrainer, XGBoostTrainer\nfrom src.model2 import persistence as persistence_module\nfrom src.model2.persistence import (\n    load_oof_predictions,\n    load_trained_model,\n    save_oof_predictions,\n    save_trained_model,\n)\n\n\ndef _build_multiindex_oof_df() -> pd.DataFrame:\n    instruments = [\"AAPL\", \"GOOGL\"]\n    dates = pd.date_range(\"2024-01-01\", periods=5, freq=\"D\")\n    index = pd.MultiIndex.from_product(\n        [instruments, dates], names=[\"instrument\", \"datetime\"]\n    )\n    predictions = np.linspace(-0.5, 0.5, len(index))\n    folds = np.tile(np.arange(5, dtype=np.int8), len(instruments))\n    df = pd.DataFrame({\"prediction\": predictions, \"fold_id\": folds}, index=index)\n    return df\n\n\n# ---------------------------------------------------------------------------\n# Model persistence tests\n# ---------------------------------------------------------------------------\n\n\ndef test_save_and_load_ridge_model_round_trip(tmp_path: Path) -> None:\n    X = pd.DataFrame(np.random.randn(128, 6), columns=[f\"f{i}\" for i in range(6)])\n    y = pd.Series(np.random.randn(128))\n\n    trainer = RidgeTrainer()\n    trainer.fit(X, y)\n\n    model_path = tmp_path / \"models\" / \"ridge_21d.pkl\"\n    saved_metadata = save_trained_model(trainer, model_path)\n\n    assert model_path.exists()\n    loaded_model, loaded_metadata = load_trained_model(model_path, expected_type=RidgeTrainer)\n\n    X_eval = X.iloc[:10]\n    np.testing.assert_allclose(trainer.predict(X_eval), loaded_model.predict(X_eval))\n    assert saved_metadata.model_hash == loaded_metadata.model_hash\n    assert loaded_metadata.class_path.endswith(\"RidgeTrainer\")\n\n\ndef test_save_and_load_xgboost_model_round_trip(tmp_path: Path) -> None:\n    rng = np.random.default_rng(42)\n    X = pd.DataFrame(rng.normal(size=(200, 8)), columns=[f\"x{i}\" for i in range(8)])\n    y = pd.Series(rng.normal(size=200))\n\n    trainer = XGBoostTrainer()\n    trainer.fit(X, y)\n\n    model_path = tmp_path / \"models\" / \"xgb_63d.pkl\"\n    saved_metadata = save_trained_model(trainer, model_path)\n\n    loaded_model, loaded_metadata = load_trained_model(model_path, expected_type=XGBoostTrainer)\n\n    X_eval = X.iloc[:20]\n    np.testing.assert_allclose(trainer.predict(X_eval), loaded_model.predict(X_eval), atol=1e-9)\n    assert saved_metadata.model_hash == loaded_metadata.model_hash\n    assert loaded_metadata.class_path.endswith(\"XGBoostTrainer\")\n\n\ndef test_save_model_creates_parent_dir_and_warns(tmp_path: Path, caplog: pytest.LogCaptureFixture) -> None:\n    X = pd.DataFrame(np.random.randn(32, 4), columns=[f\"c{i}\" for i in range(4)])\n    y = pd.Series(np.random.randn(32))\n    trainer = RidgeTrainer().fit(X, y)\n\n    model_path = tmp_path / \"nested\" / \"ridge.pkl\"\n    save_trained_model(trainer, model_path)\n    assert model_path.exists()\n\n    caplog.clear()\n    with caplog.at_level(logging.WARNING, logger=\"src.model2.persistence\"):\n        save_trained_model(trainer, model_path)\n    assert any(\"Overwriting existing artifact\" in record.message for record in caplog.records)\n\n\ndef test_load_trained_model_type_mismatch(tmp_path: Path) -> None:\n    X = pd.DataFrame(np.random.randn(16, 3), columns=[f\"f{i}\" for i in range(3)])\n    y = pd.Series(np.random.randn(16))\n    trainer = RidgeTrainer().fit(X, y)\n\n    model_path = tmp_path / \"model.pkl\"\n    save_trained_model(trainer, model_path)\n\n    with pytest.raises(TypeError, match=\"Expected model type XGBoostTrainer\"):\n        load_trained_model(model_path, expected_type=XGBoostTrainer)\n\n\ndef test_load_trained_model_missing_file(tmp_path: Path) -> None:\n    missing = tmp_path / \"missing.pkl\"\n    with pytest.raises(FileNotFoundError):\n        load_trained_model(missing)\n\n\n# ---------------------------------------------------------------------------\n# OOF persistence tests\n# ---------------------------------------------------------------------------\n\n\ndef test_save_and_load_oof_predictions_multiindex(tmp_path: Path) -> None:\n    oof_df = _build_multiindex_oof_df()\n    parquet_path = tmp_path / \"oof\" / \"ridge_21d.parquet\"\n\n    saved_metadata = save_oof_predictions(oof_df, parquet_path)\n    assert parquet_path.exists()\n\n    loaded_df, loaded_metadata = load_oof_predictions(parquet_path)\n\n    pd.testing.assert_frame_equal(loaded_df, oof_df.astype({\"prediction\": np.float32, \"fold_id\": np.int8}))\n    assert saved_metadata.data_hash == loaded_metadata.data_hash\n    assert loaded_df.index.names == (\"instrument\", \"datetime\")\n    assert loaded_df[\"prediction\"].dtype == np.float32\n    assert loaded_df[\"fold_id\"].dtype == np.int8\n\n\ndef test_save_oof_predictions_missing_columns(tmp_path: Path) -> None:\n    df = pd.DataFrame({\"prediction\": [0.1, 0.2]})\n    parquet_path = tmp_path / \"oof.parquet\"\n\n    with pytest.raises(ValueError, match=\"(?i)missing required columns\"):\n        save_oof_predictions(df, parquet_path)\n\n\ndef test_save_oof_predictions_warns_on_overwrite(tmp_path: Path, caplog: pytest.LogCaptureFixture) -> None:\n    df = _build_multiindex_oof_df()\n    parquet_path = tmp_path / \"oof.parquet\"\n    save_oof_predictions(df, parquet_path)\n    caplog.clear()\n    with caplog.at_level(logging.WARNING, logger=\"src.model2.persistence\"):\n        save_oof_predictions(df, parquet_path)\n\n    assert any(\"Overwriting existing artifact\" in record.message for record in caplog.records)\n\n\ndef test_load_oof_predictions_hash_mismatch(tmp_path: Path) -> None:\n    df = _build_multiindex_oof_df()\n    parquet_path = tmp_path / \"oof.parquet\"\n    save_oof_predictions(df, parquet_path)\n\n    table = pq.read_table(parquet_path)\n    metadata = dict(table.schema.metadata or {})\n    metadata_key = persistence_module.OOF_METADATA_KEY\n    corrupted = json.loads(metadata[metadata_key].decode(\"utf-8\"))\n    corrupted[\"data_hash\"] = \"corrupted\"\n    metadata[metadata_key] = json.dumps(corrupted).encode(\"utf-8\")\n    pq.write_table(table.replace_schema_metadata(metadata), parquet_path)\n\n    with pytest.raises(ValueError, match=\"OOF predictions hash mismatch\"):\n        load_oof_predictions(parquet_path)\n\n\ndef test_save_and_load_empty_oof_predictions(tmp_path: Path) -> None:\n    index = pd.MultiIndex.from_arrays([[], []], names=[\"instrument\", \"datetime\"])\n    df = pd.DataFrame({\"prediction\": pd.Series(dtype=np.float32), \"fold_id\": pd.Series(dtype=np.int8)}, index=index)\n    parquet_path = tmp_path / \"empty.parquet\"\n\n    save_oof_predictions(df, parquet_path)\n    loaded_df, _ = load_oof_predictions(parquet_path)\n\n    assert loaded_df.empty\n    assert isinstance(loaded_df.index, pd.MultiIndex)\n    assert loaded_df[\"prediction\"].dtype == np.float32\n    assert loaded_df[\"fold_id\"].dtype == np.int8\n"
    }
  ]
}
