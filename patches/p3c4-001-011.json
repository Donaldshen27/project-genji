{
  "ticket": "P3C4-001-011",
  "description": "Implement comprehensive determinism validation tests for Ridge and XGBoost base models. Tests verify that training with random_state=42 produces identical results across repeat runs, validating reproducibility for both OOF predictions and final models. Includes three test categories: Ridge determinism (tolerance < 1e-9), XGBoost determinism (tolerance < 1e-6), and cross-run full pipeline determinism.",
  "context": "References P3C4-001-009 (Multi-Horizon Training Wrapper). Uses RidgeTrainer, XGBoostTrainer, run_cv_training(), and PurgedEmbargoedTimeSeriesSplit from src/model2/base_models.py and src/model2/train.py. Per CLAUDE.md global seed=42 requirement, all model training must be reproducible. Synthetic dataset sized to satisfy CPCV constraints: (max(63, 21) + 1) * (3 + 1) = 256 unique trading days minimum.",
  "notes": "Fixed synthetic dataset to generate 256 business days (5 instruments × 256 days = 1280 samples) to satisfy PurgedEmbargoedTimeSeriesSplit requirements with n_splits=3, embargo_days=63, max_label_horizon=21. MultiIndex constructed in date-major order to ensure datetime level is monotonically increasing. XGBoost uses tree_method='hist' for CPU-based determinism (no GPU non-determinism). Tests compare sorted predictions to handle any ordering variations.",
  "files": [
    {
      "path": "tests/unit/test_model2_base_models.py",
      "content": "\"\"\"Unit tests for base model determinism validation (P3C4-001-011).\n\nTests:\n- test_ridge_determinism: Two runs produce identical Ridge OOF (max_diff < 1e-9)\n- test_xgboost_determinism: Two runs produce identical XGBoost OOF (max_diff < 1e-6)\n- test_cross_run_determinism: Full pipeline run twice produces identical all outputs\n\nAcceptance:\n- Determinism test passes for both Ridge and XGBoost on synthetic data\n- Identical results when training with random_state=42\n- Sorted index comparison to handle prediction order variations\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom src.model2.base_models import (\n    RidgeTrainer,\n    XGBoostTrainer,\n    run_cv_training,\n    aggregate_oof_predictions,\n)\nfrom src.model2.train import PurgedEmbargoedTimeSeriesSplit\n\n\n# ============================================================================\n# Fixtures\n# ============================================================================\n\n\n@pytest.fixture\ndef synthetic_dataset():\n    \"\"\"Create fixed synthetic dataset for determinism testing.\n\n    Returns:\n        Tuple of (X, y) where:\n        - X: DataFrame (1280 samples, 10 features) with MultiIndex (instrument, datetime)\n        - y: Series (1280 samples) with same MultiIndex\n        - Fixed seed=42 for reproducibility\n\n    Schema:\n        X columns: [feature_0, feature_1, ..., feature_9]\n        y: continuous target in [-2, 2] range\n        Index: MultiIndex[(instrument, datetime)] with 5 instruments, 256 dates each\n        Dates are sorted to satisfy PurgedEmbargoedTimeSeriesSplit monotonicity requirement\n\n    Note:\n        Dataset sized for n_splits=3, embargo_days=63, max_label_horizon=21:\n        Requires (max(63, 21) + 1) * (3 + 1) = 256 unique trading days minimum.\n        With 5 instruments × 256 dates = 1280 total samples.\n    \"\"\"\n    # Set seed for reproducibility\n    np.random.seed(42)\n\n    n_instruments = 5\n    n_dates = 256  # Minimum required for CPCV with n_splits=3, embargo=63, horizon=21\n    n_features = 10\n    n_samples = n_instruments * n_dates  # 1280 samples\n\n    # Generate dates (256 consecutive trading days)\n    base_date = pd.Timestamp('2020-01-01')\n    dates = pd.date_range(base_date, periods=n_dates, freq='B')\n\n    # Create MultiIndex: (instrument, datetime) in date-major order\n    # CRITICAL: Iterate dates first to ensure datetime level is non-decreasing\n    instruments = [f'STOCK{i:03d}' for i in range(n_instruments)]\n    index_tuples = [(inst, date) for date in dates for inst in instruments]\n    multi_index = pd.MultiIndex.from_tuples(\n        index_tuples,\n        names=['instrument', 'datetime']\n    )\n\n    # Generate features: random values in [-1, 1]\n    X = pd.DataFrame(\n        np.random.randn(n_samples, n_features),\n        index=multi_index,\n        columns=[f'feature_{i}' for i in range(n_features)]\n    )\n\n    # Generate target: linear combination of features + noise\n    true_weights = np.random.randn(n_features)\n    y = pd.Series(\n        X.values @ true_weights + np.random.randn(n_samples) * 0.1,\n        index=multi_index,\n        name='label_21d'\n    )\n\n    return X, y\n\n\n@pytest.fixture\ndef cv_splitter():\n    \"\"\"Create CPCV splitter for determinism testing.\n\n    Returns:\n        PurgedEmbargoedTimeSeriesSplit instance with:\n        - n_splits=3 (small for fast testing)\n        - max_label_horizon=21 (matches label_21d)\n        - embargo_days=63 (NON-NEGOTIABLE per specs, hardcoded in class)\n\n    Note:\n        Requires sufficient data for 4 sections (n_splits + 1).\n        With 256 dates, each section has ~64 dates, satisfying embargo=63 requirement.\n    \"\"\"\n    return PurgedEmbargoedTimeSeriesSplit(\n        n_splits=3,\n        max_label_horizon=21,\n    )\n\n\n# ============================================================================\n# Test Ridge Determinism\n# ============================================================================\n\n\nclass TestRidgeDeterminism:\n    \"\"\"Test suite for Ridge model determinism validation.\"\"\"\n\n    def test_ridge_determinism(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test Ridge training produces identical results on repeat runs.\n\n        Acceptance:\n        - Two runs with random_state=42 produce identical OOF predictions\n        - max_diff < 1e-9 (Ridge uses deterministic linear algebra)\n        - Predictions sorted by index before comparison\n\n        Edge cases:\n        - Order of predictions: Sort by index before comparison\n        - Numerical precision: Allow 1e-9 tolerance for floating point\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # First run\n        trainer1 = RidgeTrainer()  # Uses frozen random_state=42\n        result1 = run_cv_training(\n            X=X,\n            y=y,\n            cv_splitter=cv_splitter,\n            trainer=trainer1,\n            model_name='ridge',\n            horizon='21d',\n        )\n        oof1 = result1['oof_predictions'].sort_index()\n\n        # Second run\n        trainer2 = RidgeTrainer()  # Fresh instance with same frozen seed\n        result2 = run_cv_training(\n            X=X,\n            y=y,\n            cv_splitter=cv_splitter,\n            trainer=trainer2,\n            model_name='ridge',\n            horizon='21d',\n        )\n        oof2 = result2['oof_predictions'].sort_index()\n\n        # Verify OOF predictions are identical\n        assert len(oof1) == len(oof2), \"OOF prediction counts differ\"\n        assert oof1.index.equals(oof2.index), \"OOF indices differ\"\n\n        # Check prediction values (max_diff < 1e-9)\n        max_diff = np.abs(oof1['prediction'] - oof2['prediction']).max()\n        assert max_diff < 1e-9, (\n            f\"Ridge predictions not deterministic: max_diff={max_diff:.2e} >= 1e-9\"\n        )\n\n        # Check fold_id consistency\n        assert (oof1['fold_id'] == oof2['fold_id']).all(), \"Fold IDs differ\"\n\n    def test_ridge_determinism_final_model(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test Ridge final model produces identical predictions on repeat runs.\n\n        Acceptance:\n        - Two final models trained on full data produce identical predictions\n        - max_diff < 1e-9 for predictions on test data\n\n        Edge cases:\n        - Test data may differ from training data\n        - Model internal state should be identical\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # First run\n        trainer1 = RidgeTrainer()\n        result1 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer1, model_name='ridge', horizon='21d',\n        )\n        final_model1 = result1['final_model']\n\n        # Second run\n        trainer2 = RidgeTrainer()\n        result2 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer2, model_name='ridge', horizon='21d',\n        )\n        final_model2 = result2['final_model']\n\n        # Predict on full dataset\n        pred1 = final_model1.predict(X)\n        pred2 = final_model2.predict(X)\n\n        # Verify predictions are identical\n        max_diff = np.abs(pred1 - pred2).max()\n        assert max_diff < 1e-9, (\n            f\"Ridge final model predictions not deterministic: max_diff={max_diff:.2e}\"\n        )\n\n\n# ============================================================================\n# Test XGBoost Determinism\n# ============================================================================\n\n\nclass TestXGBoostDeterminism:\n    \"\"\"Test suite for XGBoost model determinism validation.\"\"\"\n\n    def test_xgboost_determinism(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test XGBoost training produces identical results on repeat runs.\n\n        Acceptance:\n        - Two runs with random_state=42 produce identical OOF predictions\n        - max_diff < 1e-6 (XGBoost has minor floating point variations)\n        - Predictions sorted by index before comparison\n\n        Edge cases:\n        - XGBoost GPU vs CPU: Only test CPU (GPU may have minor diffs)\n        - Numerical precision: Allow 1e-6 tolerance for floating point\n        - Order of predictions: Sort by index before comparison\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # First run\n        trainer1 = XGBoostTrainer()  # Uses frozen random_state=42, tree_method='hist'\n        result1 = run_cv_training(\n            X=X,\n            y=y,\n            cv_splitter=cv_splitter,\n            trainer=trainer1,\n            model_name='xgboost',\n            horizon='21d',\n        )\n        oof1 = result1['oof_predictions'].sort_index()\n\n        # Second run\n        trainer2 = XGBoostTrainer()  # Fresh instance with same frozen seed\n        result2 = run_cv_training(\n            X=X,\n            y=y,\n            cv_splitter=cv_splitter,\n            trainer=trainer2,\n            model_name='xgboost',\n            horizon='21d',\n        )\n        oof2 = result2['oof_predictions'].sort_index()\n\n        # Verify OOF predictions are identical (within tolerance)\n        assert len(oof1) == len(oof2), \"OOF prediction counts differ\"\n        assert oof1.index.equals(oof2.index), \"OOF indices differ\"\n\n        # Check prediction values (max_diff < 1e-6)\n        max_diff = np.abs(oof1['prediction'] - oof2['prediction']).max()\n        assert max_diff < 1e-6, (\n            f\"XGBoost predictions not deterministic: max_diff={max_diff:.2e} >= 1e-6\"\n        )\n\n        # Check fold_id consistency\n        assert (oof1['fold_id'] == oof2['fold_id']).all(), \"Fold IDs differ\"\n\n    def test_xgboost_determinism_final_model(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test XGBoost final model produces identical predictions on repeat runs.\n\n        Acceptance:\n        - Two final models trained on full data produce identical predictions\n        - max_diff < 1e-6 for predictions on test data\n\n        Edge cases:\n        - XGBoost tree_method='hist' should be deterministic on CPU\n        - Feature name sanitization should be consistent\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # First run\n        trainer1 = XGBoostTrainer()\n        result1 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer1, model_name='xgboost', horizon='21d',\n        )\n        final_model1 = result1['final_model']\n\n        # Second run\n        trainer2 = XGBoostTrainer()\n        result2 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer2, model_name='xgboost', horizon='21d',\n        )\n        final_model2 = result2['final_model']\n\n        # Predict on full dataset\n        pred1 = final_model1.predict(X)\n        pred2 = final_model2.predict(X)\n\n        # Verify predictions are identical (within tolerance)\n        max_diff = np.abs(pred1 - pred2).max()\n        assert max_diff < 1e-6, (\n            f\"XGBoost final model predictions not deterministic: max_diff={max_diff:.2e}\"\n        )\n\n    def test_xgboost_cpu_only(self):\n        \"\"\"Test XGBoost trainer uses CPU (tree_method='hist').\n\n        Acceptance:\n        - tree_method='hist' (CPU, deterministic)\n        - NOT tree_method='gpu_hist' (GPU may have non-deterministic behavior)\n\n        Edge case:\n        - GPU vs CPU: Only test CPU for determinism guarantees\n        \"\"\"\n        trainer = XGBoostTrainer()\n        assert trainer.model.get_params()['tree_method'] == 'hist', (\n            \"XGBoost must use tree_method='hist' for CPU-based determinism\"\n        )\n\n\n# ============================================================================\n# Test Cross-Run Determinism (Full Pipeline)\n# ============================================================================\n\n\nclass TestCrossRunDeterminism:\n    \"\"\"Test suite for full pipeline determinism validation.\"\"\"\n\n    def test_cross_run_determinism_cv_scores(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test CV scores are identical across runs.\n\n        Acceptance:\n        - Two runs produce identical CV scores (r2, mse, mae per fold)\n        - Tolerance: 1e-9 for Ridge, 1e-6 for XGBoost\n\n        Edge cases:\n        - CV scores derived from predictions, so should match prediction tolerance\n        - Fold assignment must be deterministic\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # Test Ridge CV scores\n        trainer1 = RidgeTrainer()\n        result1 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer1, model_name='ridge', horizon='21d',\n        )\n        cv_scores1 = result1['cv_scores']\n\n        trainer2 = RidgeTrainer()\n        result2 = run_cv_training(\n            X=X, y=y, cv_splitter=cv_splitter,\n            trainer=trainer2, model_name='ridge', horizon='21d',\n        )\n        cv_scores2 = result2['cv_scores']\n\n        # Verify CV scores match\n        assert len(cv_scores1) == len(cv_scores2), \"CV score counts differ\"\n        for i, (score1, score2) in enumerate(zip(cv_scores1, cv_scores2)):\n            assert score1['fold_id'] == score2['fold_id'], f\"Fold {i}: fold_id differs\"\n            assert abs(score1['r2'] - score2['r2']) < 1e-9, f\"Fold {i}: r2 differs\"\n            assert abs(score1['mse'] - score2['mse']) < 1e-9, f\"Fold {i}: mse differs\"\n            assert abs(score1['mae'] - score2['mae']) < 1e-9, f\"Fold {i}: mae differs\"\n\n    def test_cross_run_determinism_fold_assignment(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test CV fold assignment is deterministic.\n\n        Acceptance:\n        - cv_splitter.split() produces identical fold indices across runs\n        - Train/test splits match exactly\n\n        Edge cases:\n        - Fold assignment must be deterministic for fair comparison\n        - PurgedEmbargoedTimeSeriesSplit should be deterministic (no randomness)\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # First split\n        splits1 = list(cv_splitter.split(X))\n\n        # Second split\n        splits2 = list(cv_splitter.split(X))\n\n        # Verify fold counts match\n        assert len(splits1) == len(splits2), \"Fold counts differ\"\n\n        # Verify each fold's train/test indices match\n        for fold_idx, ((train1, test1), (train2, test2)) in enumerate(zip(splits1, splits2)):\n            assert np.array_equal(train1, train2), f\"Fold {fold_idx}: train indices differ\"\n            assert np.array_equal(test1, test2), f\"Fold {fold_idx}: test indices differ\"\n\n    def test_cross_run_determinism_all_outputs(self, synthetic_dataset, cv_splitter):\n        \"\"\"Test full pipeline produces identical outputs across runs.\n\n        Acceptance:\n        - OOF predictions match (max_diff < tolerance)\n        - CV scores match (max_diff < tolerance)\n        - Final model predictions match (max_diff < tolerance)\n\n        Edge cases:\n        - Test both Ridge and XGBoost in single test\n        - Verify all output artifacts are deterministic\n        \"\"\"\n        X, y = synthetic_dataset\n\n        # Ridge: Run full pipeline twice\n        ridge_results = []\n        for run_idx in range(2):\n            trainer = RidgeTrainer()\n            result = run_cv_training(\n                X=X, y=y, cv_splitter=cv_splitter,\n                trainer=trainer, model_name='ridge', horizon='21d',\n            )\n            ridge_results.append(result)\n\n        # Verify Ridge outputs match\n        oof1 = ridge_results[0]['oof_predictions'].sort_index()\n        oof2 = ridge_results[1]['oof_predictions'].sort_index()\n        assert np.abs(oof1['prediction'] - oof2['prediction']).max() < 1e-9\n\n        # XGBoost: Run full pipeline twice\n        xgb_results = []\n        for run_idx in range(2):\n            trainer = XGBoostTrainer()\n            result = run_cv_training(\n                X=X, y=y, cv_splitter=cv_splitter,\n                trainer=trainer, model_name='xgboost', horizon='21d',\n            )\n            xgb_results.append(result)\n\n        # Verify XGBoost outputs match\n        oof1 = xgb_results[0]['oof_predictions'].sort_index()\n        oof2 = xgb_results[1]['oof_predictions'].sort_index()\n        assert np.abs(oof1['prediction'] - oof2['prediction']).max() < 1e-6\n"
    }
  ]
}
