{
  "ticket": "P3C4-001-009",
  "description": "Implement multi-horizon training wrapper and save functions to orchestrate training across all model-horizon pairs (ridge/xgboost Ã— 21d/63d). Includes validation for missing labels, feature-label alignment via inner join, partial failure handling, and comprehensive save utilities for OOF predictions, models, CV scores, and feature importance.",
  "context": "Per P3C4-001-009 ticket requirements and contracts in work_items.json. Depends on P3C4-001-006 (CV training orchestrator), P3C4-001-007 (feature importance), and P3C4-001-008 (persistence utilities). Implements train_multi_horizon() and save_multi_horizon_results() functions with all edge cases covered.",
  "notes": "Implementation follows the spec: validates label columns, performs inner join on features/labels with logging, instantiates trainers directly (RidgeTrainer/XGBoostTrainer), handles partial failures, raises RuntimeError if all pairs fail. Save function creates subdirectories and persists all artifacts with proper naming. Tests cover all 4 acceptance criteria plus edge cases (missing labels, index mismatch, partial failure, all outputs saved).",
  "files": [
    {
      "path": "src/model2/base_models.py",
      "content": "\"\"\"Model 2 Base Models: Trainers and CV Orchestration.\n\nImplements BaseModelTrainer ABC, RidgeTrainer, XGBoostTrainer,\nand CV training loop orchestrator.\n\nThis module is extracted from src/model2/train.py to separate\nbase model implementations from the CPCV splitting logic.\n\"\"\"\n\nimport json\nimport logging\nimport math\nimport re\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable, Sequence\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseModelTrainer(ABC):\n    \"\"\"\n    Abstract base class for model trainers.\n\n    Provides consistent interface for Ridge, XGBoost, and future models.\n    Per P3C4-001-001: define abstract methods for training and prediction.\n\n    Methods:\n        fit(X, y): Train the model on features X and labels y\n        predict(X): Generate predictions for features X\n        get_feature_importance(): Return feature importance (or None)\n        get_params(): Return model hyperparameters\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"BaseModelTrainer\":\n        \"\"\"Train the model.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n            y: Target labels (N_samples,)\n\n        Returns:\n            Self for chaining\n\n        Raises:\n            ValueError: If X or y are empty or contain invalid values\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate predictions.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n\n        Returns:\n            Array of predictions (N_samples,)\n\n        Raises:\n            RuntimeError: If model not fitted\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract feature importance.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Returns None if model does not support feature importance\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return model hyperparameters.\n\n        Returns:\n            Dictionary of hyperparameters\n        \"\"\"\n        raise NotImplementedError\n\n\nclass RidgeTrainer(BaseModelTrainer):\n    \"\"\"\n    Ridge regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-002: alpha=3.0, random_state=42 (NON-NEGOTIABLE)\n\n    \"\"\"\n\n    ALPHA: float = 3.0\n    RANDOM_STATE: int = 42\n\n    def __init__(self, alpha: float = ALPHA, random_state: int = RANDOM_STATE):\n        \"\"\"Initialize Ridge trainer with frozen hyperparameters.\"\"\"\n        if not math.isclose(alpha, self.ALPHA, abs_tol=1e-12):\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter alpha is frozen at {self.ALPHA} (got {alpha}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = Ridge(alpha=self.ALPHA, random_state=self.RANDOM_STATE)\n        self._is_fitted = False\n\n        logger.debug(\n            \"Initialized RidgeTrainer with alpha=%s, random_state=%s (frozen values).\",\n            self.ALPHA,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"RidgeTrainer\":\n        \"\"\"Train Ridge model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in X or y: sklearn raises, propagate\n        - Singular matrix: Ridge regularization prevents this\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        self.model.fit(X, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted RidgeTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate Ridge predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"RidgeTrainer must be fitted before calling predict().\")\n\n        predictions = self.model.predict(X)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Ridge does not have feature importance.\n\n        Returns:\n            None (Ridge coefficients exist but not standardized as 'importance')\n        \"\"\"\n        return None\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return Ridge hyperparameters.\"\"\"\n        return {\n            \"alpha\": self.ALPHA,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\nclass XGBoostTrainer(BaseModelTrainer):\n    \"\"\"\n    XGBoost regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-003 and specs:\n    - max_depth=6\n    - n_estimators=400\n    - learning_rate=0.05 (eta)\n    - subsample=0.8\n    - colsample_bytree=0.8\n    - random_state=42\n    All parameters NON-NEGOTIABLE per specs Section 1.\n\n    Uses xgboost.XGBRegressor with tree_method='hist' for memory efficiency.\n    Sanitizes feature names by replacing special characters with underscores.\n    \"\"\"\n\n    MAX_DEPTH: int = 6\n    N_ESTIMATORS: int = 400\n    LEARNING_RATE: float = 0.05\n    SUBSAMPLE: float = 0.8\n    COLSAMPLE_BYTREE: float = 0.8\n    RANDOM_STATE: int = 42\n\n    def __init__(\n        self,\n        max_depth: int = 6,\n        n_estimators: int = 400,\n        learning_rate: float = 0.05,\n        subsample: float = 0.8,\n        colsample_bytree: float = 0.8,\n        random_state: int = 42,\n    ):\n        \"\"\"Initialize XGBoost trainer with frozen hyperparameters.\n\n        Args:\n            max_depth: Maximum tree depth (default 6)\n            n_estimators: Number of boosting rounds (default 400)\n            learning_rate: Step size shrinkage (default 0.05)\n            subsample: Subsample ratio of training instances (default 0.8)\n            colsample_bytree: Subsample ratio of features (default 0.8)\n            random_state: Random seed (default 42)\n\n        Raises:\n            ValueError: If any hyperparameter does not match frozen value\n        \"\"\"\n        import xgboost as xgb\n\n        # Validate frozen hyperparameters\n        if max_depth != self.MAX_DEPTH:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter max_depth is frozen at {self.MAX_DEPTH} (got {max_depth}).\"\n            )\n        if n_estimators != self.N_ESTIMATORS:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter n_estimators is frozen at {self.N_ESTIMATORS} (got {n_estimators}).\"\n            )\n        if not math.isclose(learning_rate, self.LEARNING_RATE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter learning_rate is frozen at {self.LEARNING_RATE} (got {learning_rate}).\"\n            )\n        if not math.isclose(subsample, self.SUBSAMPLE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter subsample is frozen at {self.SUBSAMPLE} (got {subsample}).\"\n            )\n        if not math.isclose(colsample_bytree, self.COLSAMPLE_BYTREE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter colsample_bytree is frozen at {self.COLSAMPLE_BYTREE} (got {colsample_bytree}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = xgb.XGBRegressor(\n            max_depth=self.MAX_DEPTH,\n            n_estimators=self.N_ESTIMATORS,\n            learning_rate=self.LEARNING_RATE,\n            subsample=self.SUBSAMPLE,\n            colsample_bytree=self.COLSAMPLE_BYTREE,\n            random_state=self.RANDOM_STATE,\n            tree_method=\"hist\",\n        )\n        self._is_fitted = False\n        self._feature_name_mapping: dict[str, str] = {}\n        self._sanitized_feature_order: list[str] = []\n\n        logger.debug(\n            \"Initialized XGBoostTrainer with max_depth=%s, n_estimators=%s, learning_rate=%s, \"\n            \"subsample=%s, colsample_bytree=%s, random_state=%s (frozen values).\",\n            self.MAX_DEPTH,\n            self.N_ESTIMATORS,\n            self.LEARNING_RATE,\n            self.SUBSAMPLE,\n            self.COLSAMPLE_BYTREE,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"XGBoostTrainer\":\n        \"\"\"Train XGBoost model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in y: XGBoost raises, propagate\n        - Feature names with special chars: Sanitize before training\n        - Duplicate sanitized names: Raise ValueError with collision details\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        sanitized_columns = []\n        sanitized_to_original: dict[str, list[str]] = {}\n\n        for col in X.columns:\n            original = str(col)\n            sanitized = re.sub(r\"[^A-Za-z0-9_]\", \"_\", original)\n            sanitized_columns.append(sanitized)\n\n            if sanitized not in sanitized_to_original:\n                sanitized_to_original[sanitized] = []\n            sanitized_to_original[sanitized].append(original)\n\n        duplicates = {\n            san: orig_list for san, orig_list in sanitized_to_original.items() if len(orig_list) > 1\n        }\n        if duplicates:\n            collision_info = []\n            for san, orig_list in list(duplicates.items())[:3]:\n                collision_info.append(f\"{orig_list} -> '{san}'\")\n            raise ValueError(\n                f\"Feature name sanitization produced {len(duplicates)} duplicate(s). \"\n                f\"XGBoost requires unique feature names. Collisions: {'; '.join(collision_info)}\"\n            )\n\n        self._feature_name_mapping = {\n            san: orig_list[0] for san, orig_list in sanitized_to_original.items()\n        }\n        self._sanitized_feature_order = sanitized_columns.copy()\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        self.model.fit(X_sanitized, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted XGBoostTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate XGBoost predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"XGBoostTrainer must be fitted before calling predict().\")\n\n        sanitized_columns = []\n        for col in X.columns:\n            sanitized_columns.append(re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(col)))\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        if self._sanitized_feature_order:\n            missing_features = set(self._sanitized_feature_order) - set(X_sanitized.columns)\n            unexpected_features = set(X_sanitized.columns) - set(self._sanitized_feature_order)\n            if missing_features or unexpected_features:\n                raise ValueError(\n                    \"Prediction features must match training features exactly. \"\n                    f\"Missing: {sorted(missing_features)}; Unexpected: {sorted(unexpected_features)}\"\n                )\n            X_sanitized = X_sanitized[self._sanitized_feature_order]\n\n        predictions = self.model.predict(X_sanitized)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract XGBoost feature importance.\n\n        Per P3C4-001-007: extract gain and weight importance, log warnings for\n        edge cases (no features, zero importance), and log top 20 features at INFO level.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Sorted by importance_gain descending\n\n        Raises:\n            RuntimeError: If model not fitted\n        \"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\n                \"XGBoostTrainer must be fitted before extracting feature importance.\"\n            )\n\n        booster = self.model.get_booster()\n\n        gain_scores = booster.get_score(importance_type=\"gain\")\n        weight_scores = booster.get_score(importance_type=\"weight\")\n\n        # Edge case: No features used (constant target or model did not use any splits)\n        if not gain_scores and not weight_scores:\n            logger.warning(\n                \"No features used by XGBoost model. This may indicate a constant target \"\n                \"or insufficient training data.\"\n            )\n            return pd.DataFrame(columns=[\"feature\", \"importance_gain\", \"importance_weight\"])\n\n        all_features = set(gain_scores.keys()) | set(weight_scores.keys())\n        rows = []\n        for sanitized_feature in all_features:\n            original_name = self._feature_name_mapping.get(sanitized_feature, sanitized_feature)\n            rows.append(\n                {\n                    \"feature\": original_name,\n                    \"importance_gain\": gain_scores.get(sanitized_feature, 0.0),\n                    \"importance_weight\": weight_scores.get(sanitized_feature, 0.0),\n                }\n            )\n\n        df = pd.DataFrame(rows)\n        df = df.sort_values(\"importance_gain\", ascending=False).reset_index(drop=True)\n\n        # Edge case: All features have zero importance\n        if (df[\"importance_gain\"] == 0.0).all():\n            logger.warning(\n                \"All features have zero importance_gain. This may indicate model underfitting \"\n                \"or improper feature encoding.\"\n            )\n\n        # Log top 20 features at INFO level\n        top_n = min(20, len(df))\n        if top_n > 0:\n            logger.info(f\"Top {top_n} features by importance_gain:\")\n            for idx in range(top_n):\n                row = df.iloc[idx]\n                logger.info(\n                    f\"  {idx + 1}. {row['feature']}: gain={row['importance_gain']:.4f}, \"\n                    f\"weight={row['importance_weight']:.0f}\"\n                )\n\n        return df\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return XGBoost hyperparameters.\"\"\"\n        return {\n            \"max_depth\": self.MAX_DEPTH,\n            \"n_estimators\": self.N_ESTIMATORS,\n            \"learning_rate\": self.LEARNING_RATE,\n            \"subsample\": self.SUBSAMPLE,\n            \"colsample_bytree\": self.COLSAMPLE_BYTREE,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\ndef save_feature_importance(importance_df: pd.DataFrame, output_path: Path) -> None:\n    \"\"\"Save feature importance DataFrame to parquet file.\n\n    Per P3C4-001-007: Helper function to persist feature importance to disk.\n\n    Args:\n        importance_df: DataFrame with columns [feature, importance_gain, importance_weight]\n        output_path: Path to output parquet file\n\n    Raises:\n        ValueError: If DataFrame schema is invalid\n        OSError: If file write fails\n\n    Edge cases:\n        - Output directory doesn't exist: Create parent directories\n        - File already exists: Overwrite with warning\n        - Empty DataFrame: Write empty parquet (valid edge case)\n    \"\"\"\n    # Validate schema\n    required_columns = {\"feature\", \"importance_gain\", \"importance_weight\"}\n    if not required_columns.issubset(importance_df.columns):\n        missing = required_columns - set(importance_df.columns)\n        raise ValueError(\n            f\"Invalid feature importance schema. Missing columns: {missing}. \"\n            f\"Expected columns: {required_columns}\"\n        )\n\n    # Create parent directory if needed\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Warn if overwriting existing file\n    if output_path.exists():\n        logger.warning(f\"Overwriting existing feature importance file: {output_path}\")\n\n    # Write to parquet\n    try:\n        importance_df.to_parquet(output_path, index=False)\n        logger.info(f\"Saved feature importance ({len(importance_df)} features) to {output_path}\")\n    except Exception as e:\n        raise OSError(f\"Failed to write feature importance to {output_path}: {e}\") from e\n\n\n# ============================================================================\n# CV Training Loop Orchestrator (P3C4-001-006)\n# ============================================================================\n\n\ndef run_cv_training(\n    X: pd.DataFrame,\n    y: pd.Series,\n    cv_splitter: Any,  # PurgedEmbargoedTimeSeriesSplit\n    trainer: BaseModelTrainer,\n    model_name: str,\n    horizon: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Orchestrate cross-validation training loop for a single model-horizon pair.\n\n    Per P3C4-001-006: train on each CV fold, collect OOF predictions,\n    compute CV scores, train final model on all data.\n\n    Args:\n        X: Feature matrix with MultiIndex (instrument, datetime)\n        y: Target labels with MultiIndex (instrument, datetime)\n        cv_splitter: CPCV splitter from Chunk 3\n        trainer: BaseModelTrainer instance (RidgeTrainer or XGBoostTrainer)\n        model_name: Model identifier (\"ridge\" or \"xgboost\")\n        horizon: Horizon identifier (\"21d\" or \"63d\")\n\n    Returns:\n        Dictionary with keys:\n        - 'oof_predictions': DataFrame with columns [prediction, fold_id]\n        - 'final_model': Trained BaseModelTrainer on full data\n        - 'cv_scores': List of dicts with per-fold metrics\n\n    Raises:\n        ValueError: If CV fold has < 100 samples or training fails\n\n    Edge cases:\n        - Small fold: Validate min 100 samples per fold\n        - Training failure: Log error with fold context, re-raise\n        - Outlier predictions: Log warning if all predictions exceed Â±1000 bps\n    \"\"\"\n    logger.info(\n        f\"Starting CV training: model={model_name}, horizon={horizon}, \"\n        f\"n_samples={len(X)}, n_features={X.shape[1]}, n_splits={cv_splitter.n_splits}\"\n    )\n\n    # Initialize OOF prediction storage\n    fold_predictions_list: list[tuple[Sequence[Any], Sequence[float], int]] = []\n\n    # Initialize CV scores list\n    cv_scores: list[dict[str, Any]] = []\n\n    # Iterate through CV folds\n    for fold_idx, (train_idx, test_idx) in enumerate(cv_splitter.split(X)):\n        logger.info(f\"Fold {fold_idx}: train_size={len(train_idx)}, test_size={len(test_idx)}\")\n\n        # Validate fold size >= 100 samples\n        if len(test_idx) < 100:\n            raise ValueError(\n                f\"Fold {fold_idx} has {len(test_idx)} test samples, minimum 100 required\"\n            )\n\n        # Clone trainer for this fold (fresh model instance)\n        fold_trainer = _clone_trainer(trainer)\n\n        # Extract train/test data\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        # Fit on train data\n        try:\n            fold_trainer.fit(X_train, y_train)\n        except Exception as e:\n            logger.error(f\"Fold {fold_idx} training failed: {e}\")\n            raise ValueError(f\"Fold {fold_idx} training failed\") from e\n\n        # Predict on test data\n        predictions = fold_trainer.predict(X_test)\n\n        # CRITICAL FIX: Store predictions with X_test.index to preserve MultiIndex\n        # This ensures downstream joins work correctly with (instrument, datetime) labels\n        fold_predictions_list.append((X_test.index, predictions, fold_idx))\n\n        # Compute and log fold metrics\n        fold_metrics = _compute_fold_metrics(\n            y_test, predictions, fold_idx, model_name, horizon\n        )\n        cv_scores.append(fold_metrics)\n        logger.info(\n            f\"Fold {fold_idx} metrics: r2={fold_metrics['r2']:.4f}, \"\n            f\"mse={fold_metrics['mse']:.4f}, mae={fold_metrics['mae']:.4f}\"\n        )\n\n        # Check for outlier predictions\n        _check_outlier_predictions(predictions, fold_idx)\n\n    # Aggregate OOF predictions\n    oof_predictions = aggregate_oof_predictions(fold_predictions_list)\n    logger.info(f\"Aggregated {len(oof_predictions)} OOF predictions across all folds\")\n\n    # Train final model on all data\n    logger.info(f\"Training final model on all {len(X)} samples\")\n    final_trainer = _clone_trainer(trainer)\n    try:\n        final_trainer.fit(X, y)\n    except Exception as e:\n        logger.error(f\"Final model training failed: {e}\")\n        raise ValueError(\"Final model training failed\") from e\n\n    # Log aggregate CV scores\n    if cv_scores:\n        r2_scores = [score[\"r2\"] for score in cv_scores]\n        logger.info(\n            f\"CV aggregate: r2_mean={np.mean(r2_scores):.4f}, r2_std={np.std(r2_scores):.4f}\"\n        )\n\n    # Return results\n    return {\n        \"oof_predictions\": oof_predictions,\n        \"final_model\": final_trainer,\n        \"cv_scores\": cv_scores,\n    }\n\n\ndef _clone_trainer(trainer: BaseModelTrainer) -> BaseModelTrainer:\n    \"\"\"\n    Clone a trainer instance to create a fresh model for a new fold.\n\n    Per P3C4-001-006: Each fold should use a fresh model instance.\n    Uses trainer.__class__() to preserve actual class (including subclasses).\n\n    Args:\n        trainer: Original trainer instance\n\n    Returns:\n        New trainer instance of the same class with default hyperparameters\n\n    Note:\n        This preserves subclass semantics, critical for testing with mock\n        trainers like FailingTrainer(RidgeTrainer).\n    \"\"\"\n    return trainer.__class__()\n\n\ndef _compute_fold_metrics(\n    y_true: pd.Series,\n    y_pred: np.ndarray,\n    fold_idx: int,\n    model_name: str,\n    horizon: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Compute CV metrics for a single fold.\n\n    Per P3C4-001-006: Compute r2, mse, mae for each fold.\n\n    Args:\n        y_true: Ground truth labels\n        y_pred: Model predictions\n        fold_idx: Fold index\n        model_name: Model identifier\n        horizon: Horizon identifier\n\n    Returns:\n        Dictionary with fold metrics: {model, horizon, fold_id, r2, mse, mae}\n\n    Raises:\n        ValueError: If y_pred contains NaN values\n    \"\"\"\n    # Validate predictions are finite\n    if not np.all(np.isfinite(y_pred)):\n        raise ValueError(f\"Fold {fold_idx}: predictions contain NaN or Inf values\")\n\n    # Compute metrics\n    r2 = r2_score(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n\n    # Handle edge case: constant target (r2 may be NaN)\n    if np.allclose(y_true, y_true.iloc[0]):\n        logger.warning(f\"Fold {fold_idx}: constant target detected, r2 may be NaN\")\n\n    # Return metrics dict\n    return {\n        \"model\": model_name,\n        \"horizon\": horizon,\n        \"fold_id\": fold_idx,\n        \"r2\": float(r2),\n        \"mse\": float(mse),\n        \"mae\": float(mae),\n    }\n\n\ndef _check_outlier_predictions(predictions: np.ndarray, fold_idx: int) -> None:\n    \"\"\"\n    Check for outlier predictions and log warnings.\n\n    Per P3C4-001-006: Log warning if all predictions exceed Â±1000 bps (Â±10.0).\n\n    Args:\n        predictions: Model predictions for a fold\n        fold_idx: Fold index for logging context\n    \"\"\"\n    if len(predictions) == 0:\n        return\n\n    outlier_threshold = 10.0\n    if np.all(np.abs(predictions) > outlier_threshold):\n        logger.warning(\n            f\"Fold {fold_idx}: all predictions exceed Â±{outlier_threshold} \"\n            f\"(max={predictions.max():.2f}, min={predictions.min():.2f})\"\n        )\n\n\ndef aggregate_oof_predictions(\n    fold_predictions: Iterable[tuple[Sequence[Any], Sequence[float], int]],\n) -> pd.DataFrame:\n    \"\"\"Aggregate out-of-fold predictions from individual cross-validation folds.\n\n    Args:\n        fold_predictions: Iterable of tuples ``(indices, predictions, fold_id)`` where\n            ``indices`` is a 1-D sequence of sample indices (can be MultiIndex),\n            ``predictions`` is a 1-D sequence aligned with ``indices``,\n            and ``fold_id`` identifies the fold.\n\n    Returns:\n        DataFrame indexed by the provided ``indices`` with columns ``prediction`` and\n        ``fold_id``.\n\n    Raises:\n        ValueError: If no folds are provided, shapes mismatch, indices overlap, or\n            predictions contain NaN/Inf values.\n    \"\"\"\n    entries = list(fold_predictions)\n    if not entries:\n        raise ValueError(\"fold_predictions cannot be empty.\")\n\n    frames: list[pd.DataFrame] = []\n    seen_indices: set[Any] = set()\n\n    for entry in entries:\n        if len(entry) != 3:\n            raise ValueError(\n                \"Each fold prediction entry must be a tuple of (indices, predictions, fold_id).\"\n            )\n\n        indices, predictions, fold_id = entry\n        index = pd.Index(indices)\n        preds_array = np.asarray(predictions, dtype=np.float32)\n\n        if preds_array.ndim != 1:\n            raise ValueError(f\"Predictions for fold {fold_id} must be one-dimensional.\")\n        if len(index) != len(preds_array):\n            raise ValueError(\n                f\"Fold {fold_id} has mismatched indices ({len(index)}) and predictions \"\n                f\"({len(preds_array)}).\"\n            )\n        if not np.isfinite(preds_array).all():\n            raise ValueError(f\"Fold {fold_id} contains NaN or infinite predictions.\")\n\n        index_list = index.tolist()\n        duplicate_indices = set(index_list) & seen_indices\n        if duplicate_indices:\n            duplicates_preview = list(duplicate_indices)[:3]\n            raise ValueError(\n                f\"Detected overlapping OOF indices between folds: {duplicates_preview}\"\n            )\n\n        seen_indices.update(index_list)\n\n        fold_frame = pd.DataFrame({\"prediction\": preds_array}, index=index)\n        fold_frame[\"fold_id\"] = np.int8(fold_id)\n        frames.append(fold_frame)\n\n        logger.debug(\n            \"Aggregated %s predictions for fold %s.\",\n            len(fold_frame),\n            fold_id,\n        )\n\n    aggregated = pd.concat(frames).sort_index()\n    aggregated = aggregated[[\"prediction\", \"fold_id\"]]\n    return aggregated\n\n\n# ============================================================================\n# Multi-Horizon Training Wrapper (P3C4-001-009)\n# ============================================================================\n\n\ndef train_multi_horizon(\n    features: pd.DataFrame,\n    labels: pd.DataFrame,\n    cv_splitter: Any,\n    config: dict[str, Any],\n) -> dict[tuple[str, str], dict[str, Any]]:\n    \"\"\"Train all base models for both 21d and 63d horizons.\n\n    Per P3C4-001-009: Orchestrate training across all model-horizon pairs,\n    handling edge cases (missing labels, index mismatches, partial failures).\n\n    Args:\n        features: Feature matrix with MultiIndex (instrument, datetime)\n        labels: Labels DataFrame with columns [label_21d, label_63d]\n        cv_splitter: CPCV splitter instance\n        config: Configuration dict with keys [models, horizons, ...]\n\n    Returns:\n        Dict mapping (model_name, horizon) -> {\n            'oof': DataFrame,\n            'model': BaseModelTrainer,\n            'cv_scores': List[dict]\n        }\n\n    Raises:\n        KeyError: If required label columns are missing\n        ValueError: If feature-label index mismatch or all models fail\n        RuntimeError: If all model-horizon pairs fail training\n\n    Edge cases:\n        - Label column missing: Raise KeyError with expected columns\n        - Feature-label index mismatch: Inner join, log dropped count\n        - Horizon fails mid-training: Log error, continue with other horizons\n        - All models fail: Raise RuntimeError with summary\n\n    Example:\n        >>> results = train_multi_horizon(features, labels, cv_splitter, config)\n        >>> ridge_21d_oof = results[('ridge', '21d')]['oof']\n        >>> xgb_63d_model = results[('xgboost', '63d')]['model']\n    \"\"\"\n    logger.info(\"Starting multi-horizon training orchestration\")\n\n    # Validate label columns exist\n    required_label_cols = {\"label_21d\", \"label_63d\"}\n    missing_labels = required_label_cols - set(labels.columns)\n    if missing_labels:\n        raise KeyError(\n            f\"Missing required label columns: {sorted(missing_labels)}. \"\n            f\"Expected columns: {sorted(required_label_cols)}\"\n        )\n\n    # Validate features and labels have compatible indices via inner join\n    original_feature_count = len(features)\n    original_label_count = len(labels)\n\n    # Perform inner join to align features and labels\n    aligned_features = features.join(labels[[]], how=\"inner\")\n    aligned_labels = labels.loc[aligned_features.index]\n\n    dropped_rows = original_feature_count - len(aligned_features)\n    if dropped_rows > 0:\n        logger.warning(\n            f\"Dropped {dropped_rows} rows due to feature-label index mismatch \"\n            f\"(features: {original_feature_count}, labels: {original_label_count}, \"\n            f\"aligned: {len(aligned_features)})\"\n        )\n\n    # Extract model names from config (default to ridge and xgboost)\n    model_names = config.get(\"models\", [\"ridge\", \"xgboost\"])\n    if not isinstance(model_names, list) or not model_names:\n        model_names = [\"ridge\", \"xgboost\"]\n        logger.warning(\n            f\"Config 'models' key missing or invalid, using defaults: {model_names}\"\n        )\n\n    # Extract horizons from config or default to 21d and 63d\n    horizons = config.get(\"horizons\", [\"21d\", \"63d\"])\n    if not isinstance(horizons, list) or not horizons:\n        horizons = [\"21d\", \"63d\"]\n        logger.warning(\n            f\"Config 'horizons' key missing or invalid, using defaults: {horizons}\"\n        )\n\n    # Model registry: map string names to trainer classes\n    model_registry = {\n        \"ridge\": RidgeTrainer,\n        \"xgboost\": XGBoostTrainer,\n    }\n\n    # Results storage and failure tracking\n    results: dict[tuple[str, str], dict[str, Any]] = {}\n    failures: list[tuple[str, str, str]] = []  # (model_name, horizon, error_msg)\n\n    # Iterate over all (model_name, horizon) pairs\n    total_pairs = len(model_names) * len(horizons)\n    logger.info(\n        f\"Training {total_pairs} model-horizon pairs: \"\n        f\"models={model_names}, horizons={horizons}\"\n    )\n\n    for model_name in model_names:\n        for horizon in horizons:\n            logger.info(f\"Training pair: model={model_name}, horizon={horizon}\")\n\n            try:\n                # Get trainer class from registry\n                if model_name not in model_registry:\n                    raise ValueError(\n                        f\"Unknown model name '{model_name}'. \"\n                        f\"Available models: {list(model_registry.keys())}\"\n                    )\n\n                trainer_class = model_registry[model_name]\n                trainer = trainer_class()\n\n                # Extract label series for this horizon\n                label_col = f\"label_{horizon}\"\n                if label_col not in aligned_labels.columns:\n                    raise KeyError(\n                        f\"Label column '{label_col}' not found in labels DataFrame. \"\n                        f\"Available columns: {list(aligned_labels.columns)}\"\n                    )\n\n                y = aligned_labels[label_col]\n\n                # Run CV training for this model-horizon pair\n                cv_result = run_cv_training(\n                    X=aligned_features,\n                    y=y,\n                    cv_splitter=cv_splitter,\n                    trainer=trainer,\n                    model_name=model_name,\n                    horizon=horizon,\n                )\n\n                # Store successful result\n                results[(model_name, horizon)] = {\n                    \"oof\": cv_result[\"oof_predictions\"],\n                    \"model\": cv_result[\"final_model\"],\n                    \"cv_scores\": cv_result[\"cv_scores\"],\n                }\n\n                logger.info(\n                    f\"Successfully trained {model_name} for {horizon} horizon \"\n                    f\"(oof_size={len(cv_result['oof_predictions'])}, \"\n                    f\"cv_folds={len(cv_result['cv_scores'])})\"\n                )\n\n            except Exception as e:\n                error_msg = str(e)\n                failures.append((model_name, horizon, error_msg))\n                logger.error(\n                    f\"Training failed for model={model_name}, horizon={horizon}: {error_msg}\"\n                )\n                # Continue with other pairs (partial failure handling)\n\n    # Validate at least one pair succeeded\n    n_success = len(results)\n    n_failed = len(failures)\n\n    if n_success == 0:\n        # All pairs failed - raise RuntimeError with summary\n        failure_summary = \"\\n\".join(\n            [f\"  - {model}/{horizon}: {err}\" for model, horizon, err in failures]\n        )\n        raise RuntimeError(\n            f\"All {total_pairs} model-horizon pairs failed training:\\n{failure_summary}\"\n        )\n\n    # Log summary\n    logger.info(\n        f\"Multi-horizon training complete: {n_success}/{total_pairs} pairs successful, \"\n        f\"{n_failed} failed\"\n    )\n\n    if failures:\n        logger.warning(f\"Failed pairs: {[(m, h) for m, h, _ in failures]}\")\n\n    return results\n\n\ndef save_multi_horizon_results(\n    results: dict[tuple[str, str], dict[str, Any]],\n    output_dir: Path,\n    region: str,\n) -> None:\n    \"\"\"Save all multi-horizon training outputs to disk.\n\n    Per P3C4-001-009: Persist OOF predictions, models, CV scores, and feature\n    importance for all model-horizon pairs.\n\n    Args:\n        results: Output from train_multi_horizon()\n        output_dir: Base directory for outputs (e.g., data/model2/us/)\n        region: Region identifier (\"US\" or \"CN\")\n\n    Raises:\n        OSError: If any file write operations fail\n        ValueError: If results dict is empty\n\n    Edge cases:\n        - Empty results: Raise ValueError\n        - Output directory doesn't exist: Create with parents\n        - Partial save failure: Log error, continue with remaining outputs\n\n    Output structure:\n        {output_dir}/\n            oof/\n                ridge_21d_oof.parquet\n                ridge_63d_oof.parquet\n                xgboost_21d_oof.parquet\n                xgboost_63d_oof.parquet\n            models/\n                ridge_21d.pkl\n                ridge_63d.pkl\n                xgboost_21d.pkl\n                xgboost_63d.pkl\n            cv_scores/\n                ridge_21d_cv_scores.json\n                ridge_63d_cv_scores.json\n                xgboost_21d_cv_scores.json\n                xgboost_63d_cv_scores.json\n            feature_importance/\n                xgboost_21d_importance.parquet\n                xgboost_63d_importance.parquet\n    \"\"\"\n    # Import persistence utilities here to avoid circular imports\n    from src.model2.persistence import save_oof_predictions, save_trained_model\n\n    # Validate results dict is not empty\n    if not results:\n        raise ValueError(\"Cannot save empty results dictionary\")\n\n    logger.info(\n        f\"Saving multi-horizon results for {len(results)} model-horizon pairs \"\n        f\"to {output_dir}\"\n    )\n\n    # Create output subdirectories\n    output_dir = Path(output_dir)\n    subdirs = {\n        \"oof\": output_dir / \"oof\",\n        \"models\": output_dir / \"models\",\n        \"cv_scores\": output_dir / \"cv_scores\",\n        \"feature_importance\": output_dir / \"feature_importance\",\n    }\n\n    for subdir_name, subdir_path in subdirs.items():\n        subdir_path.mkdir(parents=True, exist_ok=True)\n        logger.debug(f\"Created directory: {subdir_path}\")\n\n    # Track save successes and failures\n    save_failures: list[tuple[str, str, str, str]] = []  # (model, horizon, artifact, error)\n\n    # Iterate over all (model_name, horizon) pairs\n    for (model_name, horizon), pair_results in results.items():\n        logger.info(f\"Saving outputs for {model_name}_{horizon}\")\n\n        # 1. Save OOF predictions\n        try:\n            oof_path = subdirs[\"oof\"] / f\"{model_name}_{horizon}_oof.parquet\"\n            save_oof_predictions(pair_results[\"oof\"], oof_path)\n            logger.debug(f\"Saved OOF predictions to {oof_path}\")\n        except Exception as e:\n            error_msg = f\"Failed to save OOF predictions: {e}\"\n            save_failures.append((model_name, horizon, \"oof\", error_msg))\n            logger.error(f\"{model_name}_{horizon}: {error_msg}\")\n\n        # 2. Save final model\n        try:\n            model_path = subdirs[\"models\"] / f\"{model_name}_{horizon}.pkl\"\n            save_trained_model(\n                pair_results[\"model\"],\n                model_path,\n                extra_metadata={\"region\": region, \"horizon\": horizon},\n            )\n            logger.debug(f\"Saved model to {model_path}\")\n        except Exception as e:\n            error_msg = f\"Failed to save model: {e}\"\n            save_failures.append((model_name, horizon, \"model\", error_msg))\n            logger.error(f\"{model_name}_{horizon}: {error_msg}\")\n\n        # 3. Save CV scores to JSON\n        try:\n            cv_scores_path = subdirs[\"cv_scores\"] / f\"{model_name}_{horizon}_cv_scores.json\"\n            with open(cv_scores_path, \"w\") as f:\n                json.dump(pair_results[\"cv_scores\"], f, indent=2)\n            logger.debug(f\"Saved CV scores to {cv_scores_path}\")\n        except Exception as e:\n            error_msg = f\"Failed to save CV scores: {e}\"\n            save_failures.append((model_name, horizon, \"cv_scores\", error_msg))\n            logger.error(f\"{model_name}_{horizon}: {error_msg}\")\n\n        # 4. Save feature importance (XGBoost only)\n        if model_name == \"xgboost\":\n            try:\n                model = pair_results[\"model\"]\n                importance_df = model.get_feature_importance()\n\n                if importance_df is not None and not importance_df.empty:\n                    importance_path = (\n                        subdirs[\"feature_importance\"]\n                        / f\"{model_name}_{horizon}_importance.parquet\"\n                    )\n                    save_feature_importance(importance_df, importance_path)\n                    logger.debug(f\"Saved feature importance to {importance_path}\")\n                else:\n                    logger.warning(\n                        f\"{model_name}_{horizon}: No feature importance to save \"\n                        \"(empty or None)\"\n                    )\n            except Exception as e:\n                error_msg = f\"Failed to save feature importance: {e}\"\n                save_failures.append((model_name, horizon, \"feature_importance\", error_msg))\n                logger.error(f\"{model_name}_{horizon}: {error_msg}\")\n\n    # Log summary\n    total_artifacts = len(results) * 3 + sum(\n        1 for (model_name, _) in results.keys() if model_name == \"xgboost\"\n    )\n    n_failures = len(save_failures)\n    n_success = total_artifacts - n_failures\n\n    logger.info(\n        f\"Saved outputs for {len(results)} model-horizon pairs to {output_dir}: \"\n        f\"{n_success}/{total_artifacts} artifacts saved successfully\"\n    )\n\n    if save_failures:\n        logger.warning(\n            f\"{n_failures} artifact save failures: \"\n            f\"{[(m, h, a) for m, h, a, _ in save_failures]}\"\n        )\n"
    }
  ],
  "tests": [
    {
      "path": "tests/unit/test_model2_multi_horizon.py",
      "content": "\"\"\"Unit tests for multi-horizon training wrapper (P3C4-001-009).\n\nTests:\n- test_multi_horizon_training_success: All 4 model-horizon pairs train successfully\n- test_multi_horizon_feature_label_join: Inner join on indices, log dropped rows\n- test_multi_horizon_partial_failure: Continue on single model failure (if configured)\n- test_multi_horizon_all_outputs: Verify all 4 OOF files and 4 model files saved\n- test_multi_horizon_missing_label: Raise KeyError for missing label column\n- test_multi_horizon_all_failures: Raise RuntimeError if all pairs fail\n\nAll tests implement full acceptance criteria from P3C4-001-009 ticket.\n\"\"\"\n\nimport json\nimport logging\nfrom pathlib import Path\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom src.model2.base_models import (\n    RidgeTrainer,\n    XGBoostTrainer,\n    save_multi_horizon_results,\n    train_multi_horizon,\n)\nfrom src.model2.train import PurgedEmbargoedTimeSeriesSplit\n\n\n@pytest.fixture\ndef synthetic_multi_horizon_data():\n    \"\"\"Create synthetic data for multi-horizon training tests.\n\n    Returns features and labels with MultiIndex (datetime, instrument).\n    Labels include both label_21d and label_63d columns.\n    \"\"\"\n    np.random.seed(42)\n\n    # Create 300 trading days (sufficient for 3 folds with 63-day embargo)\n    dates = pd.date_range(\"2020-01-01\", periods=300, freq=\"D\")\n    instruments = [\"AAPL\", \"MSFT\"]\n\n    # Build MultiIndex with datetime FIRST to ensure monotonic ordering\n    index = pd.MultiIndex.from_product([dates, instruments], names=[\"datetime\", \"instrument\"])\n\n    # Create features\n    n_samples = len(index)\n    X = pd.DataFrame(\n        np.random.randn(n_samples, 5), index=index, columns=[f\"f{i}\" for i in range(5)]\n    )\n\n    # Create labels with both horizons\n    labels = pd.DataFrame(\n        {\n            \"label_21d\": np.random.randn(n_samples),\n            \"label_63d\": np.random.randn(n_samples),\n        },\n        index=index,\n    )\n\n    return X, labels\n\n\n@pytest.fixture\ndef mismatched_index_data():\n    \"\"\"Create data with feature-label index mismatch for testing inner join.\n\n    Features have 500 rows, labels have 480 rows (20 missing).\n    \"\"\"\n    np.random.seed(42)\n\n    dates_features = pd.date_range(\"2020-01-01\", periods=250, freq=\"D\")\n    dates_labels = pd.date_range(\"2020-01-11\", periods=240, freq=\"D\")  # 10 days offset\n    instruments = [\"AAPL\", \"MSFT\"]\n\n    index_features = pd.MultiIndex.from_product(\n        [dates_features, instruments], names=[\"datetime\", \"instrument\"]\n    )\n    index_labels = pd.MultiIndex.from_product(\n        [dates_labels, instruments], names=[\"datetime\", \"instrument\"]\n    )\n\n    X = pd.DataFrame(\n        np.random.randn(len(index_features), 5),\n        index=index_features,\n        columns=[f\"f{i}\" for i in range(5)],\n    )\n    labels = pd.DataFrame(\n        {\n            \"label_21d\": np.random.randn(len(index_labels)),\n            \"label_63d\": np.random.randn(len(index_labels)),\n        },\n        index=index_labels,\n    )\n\n    return X, labels\n\n\nclass TestMultiHorizonTraining:\n    \"\"\"Test suite for train_multi_horizon() function.\"\"\"\n\n    def test_multi_horizon_training_success(self, synthetic_multi_horizon_data):\n        \"\"\"Test all 4 model-horizon pairs train successfully.\n\n        Acceptance:\n        - Results dict has keys: (ridge, 21d), (ridge, 63d), (xgboost, 21d), (xgboost, 63d)\n        - Each result has keys: ['oof', 'model', 'cv_scores']\n        - OOF predictions are DataFrame with columns [prediction, fold_id]\n        - Model is fitted BaseModelTrainer instance\n        - CV scores is list of dicts with keys [model, horizon, fold_id, r2, mse, mae]\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        # Create CPCV splitter\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n\n        # Config with models and horizons\n        config = {\n            \"models\": [\"ridge\", \"xgboost\"],\n            \"horizons\": [\"21d\", \"63d\"],\n        }\n\n        # Train all model-horizon pairs\n        results = train_multi_horizon(X, labels, cv_splitter, config)\n\n        # Verify all 4 pairs are present\n        expected_keys = [\n            (\"ridge\", \"21d\"),\n            (\"ridge\", \"63d\"),\n            (\"xgboost\", \"21d\"),\n            (\"xgboost\", \"63d\"),\n        ]\n        assert set(results.keys()) == set(expected_keys)\n\n        # Verify structure of each result\n        for (model_name, horizon), result in results.items():\n            # Check keys exist\n            assert \"oof\" in result\n            assert \"model\" in result\n            assert \"cv_scores\" in result\n\n            # Verify OOF predictions\n            oof_df = result[\"oof\"]\n            assert isinstance(oof_df, pd.DataFrame)\n            assert \"prediction\" in oof_df.columns\n            assert \"fold_id\" in oof_df.columns\n            assert len(oof_df) > 0\n            assert np.all(np.isfinite(oof_df[\"prediction\"]))\n\n            # Verify model is fitted\n            model = result[\"model\"]\n            if model_name == \"ridge\":\n                assert isinstance(model, RidgeTrainer)\n            elif model_name == \"xgboost\":\n                assert isinstance(model, XGBoostTrainer)\n            assert model._is_fitted is True\n\n            # Verify CV scores\n            cv_scores = result[\"cv_scores\"]\n            assert isinstance(cv_scores, list)\n            assert len(cv_scores) == 3  # 3 folds\n            for score in cv_scores:\n                assert score[\"model\"] == model_name\n                assert score[\"horizon\"] == horizon\n                assert \"fold_id\" in score\n                assert \"r2\" in score\n                assert \"mse\" in score\n                assert \"mae\" in score\n\n    def test_multi_horizon_feature_label_join(self, mismatched_index_data, caplog):\n        \"\"\"Test inner join on features and labels with index mismatch.\n\n        Acceptance:\n        - Features have 500 rows, labels have 480 rows (20 missing)\n        - Inner join produces 480 aligned samples\n        - Warning logged: 'Dropped 20 rows due to index mismatch'\n        - Training proceeds on 480 samples\n        \"\"\"\n        X, labels = mismatched_index_data\n\n        # Verify initial sizes\n        assert len(X) == 500\n        assert len(labels) == 480\n\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n        config = {\"models\": [\"ridge\"], \"horizons\": [\"21d\"]}\n\n        # Train with logging capture\n        with caplog.at_level(logging.WARNING, logger=\"src.model2.base_models\"):\n            results = train_multi_horizon(X, labels, cv_splitter, config)\n\n        # Verify warning was logged\n        assert \"Dropped\" in caplog.text\n        assert \"index mismatch\" in caplog.text\n\n        # Verify training succeeded on aligned data\n        assert (\"ridge\", \"21d\") in results\n        oof_df = results[(\"ridge\", \"21d\")][\"oof\"]\n        # OOF size will be less than 480 due to CV purging/embargo, but should be > 0\n        assert len(oof_df) > 0\n        assert len(oof_df) <= 480\n\n    def test_multi_horizon_partial_failure(self, synthetic_multi_horizon_data, caplog):\n        \"\"\"Test continues on single model failure if config allows.\n\n        Acceptance:\n        - Mock XGBoostTrainer to raise ValueError during fit\n        - Ridge models train successfully for both horizons\n        - XGBoost failures logged: 'Model xgboost horizon 21d training failed'\n        - Results dict has only (ridge, 21d) and (ridge, 63d)\n        - No RuntimeError raised (partial success allowed)\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        # Create a failing XGBoost trainer class\n        class FailingXGBoostTrainer(XGBoostTrainer):\n            def fit(self, X, y):\n                raise ValueError(\"Simulated XGBoost training failure\")\n\n        # Monkey-patch the module to use failing XGBoost\n        import src.model2.base_models as base_models_module\n\n        original_xgboost = base_models_module.XGBoostTrainer\n        base_models_module.XGBoostTrainer = FailingXGBoostTrainer\n\n        try:\n            cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n            config = {\"models\": [\"ridge\", \"xgboost\"], \"horizons\": [\"21d\", \"63d\"]}\n\n            with caplog.at_level(logging.ERROR, logger=\"src.model2.base_models\"):\n                results = train_multi_horizon(X, labels, cv_splitter, config)\n\n            # Verify only Ridge models succeeded\n            assert set(results.keys()) == {(\"ridge\", \"21d\"), (\"ridge\", \"63d\")}\n\n            # Verify XGBoost failures were logged\n            assert \"xgboost\" in caplog.text.lower()\n            assert \"failed\" in caplog.text.lower()\n\n            # Verify Ridge models are valid\n            for horizon in [\"21d\", \"63d\"]:\n                assert isinstance(results[(\"ridge\", horizon)][\"model\"], RidgeTrainer)\n\n        finally:\n            # Restore original XGBoostTrainer\n            base_models_module.XGBoostTrainer = original_xgboost\n\n    def test_multi_horizon_all_outputs(self, synthetic_multi_horizon_data, tmp_path):\n        \"\"\"Test all outputs saved to correct directories.\n\n        Acceptance:\n        - 4 OOF parquet files in {output_dir}/oof/\n        - 4 model pickle files in {output_dir}/models/\n        - 4 CV score JSON files in {output_dir}/cv_scores/\n        - 2 feature importance parquet files in {output_dir}/feature_importance/ (XGBoost only)\n        - All files named correctly: {model}_{horizon}_*.{ext}\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n        config = {\"models\": [\"ridge\", \"xgboost\"], \"horizons\": [\"21d\", \"63d\"]}\n\n        # Train all pairs\n        results = train_multi_horizon(X, labels, cv_splitter, config)\n\n        # Save all results\n        output_dir = tmp_path / \"model2\" / \"us\"\n        save_multi_horizon_results(results, output_dir, region=\"US\")\n\n        # Verify directory structure\n        assert (output_dir / \"oof\").exists()\n        assert (output_dir / \"models\").exists()\n        assert (output_dir / \"cv_scores\").exists()\n        assert (output_dir / \"feature_importance\").exists()\n\n        # Verify OOF files (4 total)\n        expected_oof_files = [\n            \"ridge_21d_oof.parquet\",\n            \"ridge_63d_oof.parquet\",\n            \"xgboost_21d_oof.parquet\",\n            \"xgboost_63d_oof.parquet\",\n        ]\n        for filename in expected_oof_files:\n            assert (output_dir / \"oof\" / filename).exists()\n\n        # Verify model files (4 total)\n        expected_model_files = [\n            \"ridge_21d.pkl\",\n            \"ridge_63d.pkl\",\n            \"xgboost_21d.pkl\",\n            \"xgboost_63d.pkl\",\n        ]\n        for filename in expected_model_files:\n            assert (output_dir / \"models\" / filename).exists()\n\n        # Verify CV score JSON files (4 total)\n        expected_cv_files = [\n            \"ridge_21d_cv_scores.json\",\n            \"ridge_63d_cv_scores.json\",\n            \"xgboost_21d_cv_scores.json\",\n            \"xgboost_63d_cv_scores.json\",\n        ]\n        for filename in expected_cv_files:\n            cv_file = output_dir / \"cv_scores\" / filename\n            assert cv_file.exists()\n            # Verify JSON is valid\n            with open(cv_file) as f:\n                cv_scores = json.load(f)\n                assert isinstance(cv_scores, list)\n                assert len(cv_scores) == 3  # 3 folds\n\n        # Verify feature importance files (2 total, XGBoost only)\n        expected_importance_files = [\n            \"xgboost_21d_importance.parquet\",\n            \"xgboost_63d_importance.parquet\",\n        ]\n        for filename in expected_importance_files:\n            importance_file = output_dir / \"feature_importance\" / filename\n            assert importance_file.exists()\n            # Verify parquet is readable\n            importance_df = pd.read_parquet(importance_file)\n            assert \"feature\" in importance_df.columns\n            assert \"importance_gain\" in importance_df.columns\n            assert \"importance_weight\" in importance_df.columns\n\n    def test_multi_horizon_missing_label(self, synthetic_multi_horizon_data):\n        \"\"\"Test raises KeyError for missing label column.\n\n        Acceptance:\n        - Labels DataFrame has only [label_21d] (missing label_63d)\n        - train_multi_horizon() raises KeyError\n        - Error message: 'Missing required label columns: {label_63d}'\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        # Remove label_63d column\n        labels_missing = labels[[\"label_21d\"]].copy()\n\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n        config = {\"models\": [\"ridge\"], \"horizons\": [\"21d\", \"63d\"]}\n\n        # Should raise KeyError for missing label column\n        with pytest.raises(KeyError, match=\"Missing required label columns.*label_63d\"):\n            train_multi_horizon(X, labels_missing, cv_splitter, config)\n\n    def test_multi_horizon_all_failures(self, synthetic_multi_horizon_data):\n        \"\"\"Test raises RuntimeError if all model-horizon pairs fail.\n\n        Acceptance:\n        - Mock all trainers to raise ValueError during fit\n        - train_multi_horizon() raises RuntimeError\n        - Error message contains failure summary: 'All 4 model-horizon pairs failed'\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        # Create failing trainers\n        class FailingRidgeTrainer(RidgeTrainer):\n            def fit(self, X, y):\n                raise ValueError(\"Simulated Ridge failure\")\n\n        class FailingXGBoostTrainer(XGBoostTrainer):\n            def fit(self, X, y):\n                raise ValueError(\"Simulated XGBoost failure\")\n\n        # Monkey-patch both trainers\n        import src.model2.base_models as base_models_module\n\n        original_ridge = base_models_module.RidgeTrainer\n        original_xgboost = base_models_module.XGBoostTrainer\n        base_models_module.RidgeTrainer = FailingRidgeTrainer\n        base_models_module.XGBoostTrainer = FailingXGBoostTrainer\n\n        try:\n            cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n            config = {\"models\": [\"ridge\", \"xgboost\"], \"horizons\": [\"21d\", \"63d\"]}\n\n            # Should raise RuntimeError when all pairs fail\n            with pytest.raises(RuntimeError, match=\"All .* model-horizon pairs failed\"):\n                train_multi_horizon(X, labels, cv_splitter, config)\n\n        finally:\n            # Restore original trainers\n            base_models_module.RidgeTrainer = original_ridge\n            base_models_module.XGBoostTrainer = original_xgboost\n\n\nclass TestSaveMultiHorizonResults:\n    \"\"\"Test suite for save_multi_horizon_results() function.\"\"\"\n\n    def test_save_all_outputs(self, synthetic_multi_horizon_data, tmp_path):\n        \"\"\"Test all outputs saved to correct locations.\n\n        Acceptance:\n        - All subdirectories created: oof/, models/, cv_scores/, feature_importance/\n        - All files saved with correct naming: {model}_{horizon}_*.{ext}\n        - File counts: 4 OOF, 4 models, 4 CV scores, 2 feature importance\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n        config = {\"models\": [\"ridge\", \"xgboost\"], \"horizons\": [\"21d\", \"63d\"]}\n\n        results = train_multi_horizon(X, labels, cv_splitter, config)\n\n        output_dir = tmp_path / \"model2\" / \"test\"\n        save_multi_horizon_results(results, output_dir, region=\"TEST\")\n\n        # Verify all subdirectories exist\n        assert (output_dir / \"oof\").is_dir()\n        assert (output_dir / \"models\").is_dir()\n        assert (output_dir / \"cv_scores\").is_dir()\n        assert (output_dir / \"feature_importance\").is_dir()\n\n        # Count files in each directory\n        oof_files = list((output_dir / \"oof\").glob(\"*.parquet\"))\n        model_files = list((output_dir / \"models\").glob(\"*.pkl\"))\n        cv_files = list((output_dir / \"cv_scores\").glob(\"*.json\"))\n        importance_files = list((output_dir / \"feature_importance\").glob(\"*.parquet\"))\n\n        assert len(oof_files) == 4\n        assert len(model_files) == 4\n        assert len(cv_files) == 4\n        assert len(importance_files) == 2  # XGBoost only\n\n    def test_save_empty_results(self, tmp_path):\n        \"\"\"Test raises ValueError for empty results dict.\n\n        Acceptance:\n        - Empty results dict raises ValueError\n        - Error message: 'Cannot save empty results'\n        \"\"\"\n        empty_results = {}\n        output_dir = tmp_path / \"model2\" / \"test\"\n\n        with pytest.raises(ValueError, match=\"Cannot save empty results\"):\n            save_multi_horizon_results(empty_results, output_dir, region=\"TEST\")\n\n    def test_save_creates_directories(self, synthetic_multi_horizon_data, tmp_path):\n        \"\"\"Test creates output directories if they don't exist.\n\n        Acceptance:\n        - Output directory does not exist initially\n        - save_multi_horizon_results() creates all subdirectories\n        - All files saved successfully\n        \"\"\"\n        X, labels = synthetic_multi_horizon_data\n\n        cv_splitter = PurgedEmbargoedTimeSeriesSplit(n_splits=3, max_label_horizon=21)\n        config = {\"models\": [\"ridge\"], \"horizons\": [\"21d\"]}\n\n        results = train_multi_horizon(X, labels, cv_splitter, config)\n\n        # Use non-existent nested directory\n        output_dir = tmp_path / \"nested\" / \"model2\" / \"test\"\n        assert not output_dir.exists()\n\n        # Save should create all directories\n        save_multi_horizon_results(results, output_dir, region=\"TEST\")\n\n        # Verify directories were created\n        assert output_dir.exists()\n        assert (output_dir / \"oof\").exists()\n        assert (output_dir / \"models\").exists()\n        assert (output_dir / \"cv_scores\").exists()\n\n        # Verify files were saved\n        assert (output_dir / \"oof\" / \"ridge_21d_oof.parquet\").exists()\n        assert (output_dir / \"models\" / \"ridge_21d.pkl\").exists()\n        assert (output_dir / \"cv_scores\" / \"ridge_21d_cv_scores.json\").exists()\n"
    }
  ]
}
