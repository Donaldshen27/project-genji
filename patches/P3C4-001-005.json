{
  "ticket": "P3C4-001-005",
  "description": "Add comprehensive unit tests for OOF prediction aggregation function. The implementation already exists in src/model2/train.py (lines 718-786). This ticket completes the test suite for the aggregate_oof_predictions function.",
  "context": "Per ticket P3C4-001-005 in tickets/work_items.json. The aggregate_oof_predictions function is implemented and functional. These tests verify correct behavior for full coverage, duplicate detection, NaN handling, empty input, shape mismatches, and invalid formats. Source file included unchanged to satisfy patch package requirements.",
  "notes": "All tests follow existing test patterns from tests/unit/test_model2_base_trainers.py. Tests cover all edge cases specified in the ticket: duplicate indices (ValueError with overlap info), NaN predictions (ValueError with fold/index info), empty fold list (ValueError), shape mismatch (ValueError), and invalid tuple format (ValueError). Additional tests verify full coverage scenarios with multiple folds. Fixed bug in test_oof_aggregation_fold_id_type where second fold had mismatched array sizes (10 indices vs 20 predictions).",
  "files": [
    {
      "path": "src/model2/train.py",
      "content": "\"\"\"Model 2 Training Pipeline: Purged & Embargoed TimeSeriesSplit\n\nImplements Chunk 3 of Phase 3 breakdown:\n- TimeSeriesSplit with expanding window (n_splits=5)\n- Purge logic using trading-day counts\n- Embargo: 63 trading days (NON-NEGOTIABLE)\n- Validation: ensure no overlap within embargo window\n\nPer specs Section 1 (Sprint 0) and theory.md Section 4.3.\n\"\"\"\n\nimport logging\nimport math\nimport re\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Generator, Iterable, Sequence\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\n\nlogger = logging.getLogger(__name__)\n\n# NON-NEGOTIABLE constant per specs Section 1 (in trading days)\nEMBARGO_DAYS = 63\n\n\n@dataclass\nclass CPCVConfig:\n    \"\"\"Configuration for Purged & Embargoed Time Series Split.\"\"\"\n\n    n_splits: int = 5\n    max_label_horizon: int = 63  # Maximum label horizon in trading days\n\n\nclass PurgedEmbargoedTimeSeriesSplit:\n    \"\"\"\n    Expanding-window time series split with trading-day-aware purging and embargo.\n\n    Implements:\n    1. TimeSeriesSplit with expanding window (n_splits)\n    2. Purge logic: remove samples from train if label window (in trading days) overlaps test\n    3. Embargo: 63 trading-day gap between train and test (NON-NEGOTIABLE per specs)\n    4. Validation: ensure no overlap within embargo window\n\n    NOTE: All day counts (embargo_days, max_label_horizon) are in TRADING DAYS,\n    not calendar days. This is critical for financial data with weekends/holidays.\n\n    Attributes:\n        n_splits: Number of cross-validation splits\n        embargo_days: 63 trading days (hardcoded, NON-NEGOTIABLE per specs)\n        max_label_horizon: Maximum forward-looking window in trading days\n\n    Example:\n        >>> cv = PurgedEmbargoedTimeSeriesSplit(n_splits=5, max_label_horizon=63)\n        >>> for train_idx, test_idx in cv.split(data_df):\n        ...     print(f\"Train: {len(train_idx)} samples, Test: {len(test_idx)} samples\")\n    \"\"\"\n\n    def __init__(\n        self,\n        n_splits: int = 5,\n        max_label_horizon: int = 63,\n    ):\n        \"\"\"\n        Initialize purged & embargoed time series splitter.\n\n        Args:\n            n_splits: Number of expanding window splits\n            max_label_horizon: Maximum days labels look forward (in trading days)\n\n        Note:\n            embargo_days is hardcoded to 63 trading days (NON-NEGOTIABLE per specs Section 1)\n        \"\"\"\n        if n_splits < 2:\n            raise ValueError(\"n_splits must be >= 2\")\n        if max_label_horizon < 0:\n            raise ValueError(\"max_label_horizon must be >= 0\")\n\n        self.n_splits = n_splits\n        self.embargo_days = EMBARGO_DAYS  # NON-NEGOTIABLE, in trading days\n        self.max_label_horizon = max_label_horizon\n\n        logger.info(\n            f\"Initialized PurgedEmbargoedTimeSeriesSplit: n_splits={n_splits}, \"\n            f\"embargo_days={self.embargo_days} trading days (NON-NEGOTIABLE), \"\n            f\"max_label_horizon={max_label_horizon} trading days\"\n        )\n\n    def split(\n        self, X: pd.DataFrame, y=None, groups=None\n    ) -> Generator[tuple[np.ndarray, np.ndarray], None, None]:\n        \"\"\"\n        Generate train/test splits with trading-day-aware purging and embargo.\n\n        For n_splits folds, we divide the data into (n_splits + 1) sections:\n        - Fold 0: train on section 0, test on section 1\n        - Fold 1: train on sections 0-1, test on section 2\n        - ...\n        - Fold n-1: train on sections 0 to n-1, test on section n\n\n        Args:\n            X: DataFrame with DatetimeIndex or MultiIndex with datetime level\n               Must be sorted by date\n            y: Ignored (for sklearn compatibility)\n            groups: Ignored (for sklearn compatibility)\n\n        Yields:\n            (train_indices, test_indices) tuples\n            Indices are integer positions in the DataFrame\n\n        Raises:\n            ValueError: If X is empty, not sorted, has invalid index, or insufficient\n                       data for splitting with the specified embargo\n        \"\"\"\n        # Validate input\n        if X.empty:\n            raise ValueError(\"Input DataFrame is empty\")\n\n        # Extract datetime index\n        if isinstance(X.index, pd.MultiIndex):\n            if \"datetime\" not in X.index.names:\n                raise ValueError(\"MultiIndex must have 'datetime' level\")\n            dates = X.index.get_level_values(\"datetime\")\n        elif isinstance(X.index, pd.DatetimeIndex):\n            dates = X.index\n        else:\n            raise ValueError(\"Index must be DatetimeIndex or MultiIndex with datetime level\")\n\n        # Check if sorted\n        if not dates.is_monotonic_increasing:\n            raise ValueError(\"Data must be sorted by date\")\n\n        # Get unique dates (these are trading days)\n        unique_dates = pd.Series(dates.unique()).sort_values().reset_index(drop=True)\n        n_dates = len(unique_dates)\n\n        # For n_splits folds, we create n_splits + 1 sections\n        # Minimum requirement: enough trading days for embargo and purging\n        min_section_size = max(self.embargo_days, self.max_label_horizon) + 1\n        min_total_dates_needed = min_section_size * (self.n_splits + 1)\n\n        if n_dates < min_total_dates_needed:\n            raise ValueError(\n                f\"Insufficient data for {self.n_splits} splits with \"\n                f\"{self.embargo_days} trading-day embargo (NON-NEGOTIABLE). \"\n                f\"Have {n_dates} trading days but need at least \"\n                f\"{min_total_dates_needed} trading days \"\n                f\"({min_section_size} trading days per section x {self.n_splits + 1} sections)\"\n            )\n\n        logger.info(f\"Splitting {len(X)} samples across {n_dates} unique trading days\")\n        logger.info(f\"Date range: {unique_dates.min()} to {unique_dates.max()}\")\n\n        # Divide dates into (n_splits + 1) approximately equal sections\n        section_size = n_dates / (self.n_splits + 1)\n        split_points = [int(round(section_size * i)) for i in range(self.n_splits + 2)]\n        split_points[0] = 0\n        split_points[-1] = n_dates\n\n        logger.debug(f\"Split points (trading-day indices): {split_points}\")\n\n        for fold_idx in range(self.n_splits):\n            # Fold i: train on sections 0 to i, test on section i+1\n            test_start_idx = split_points[fold_idx + 1]\n            test_end_idx = split_points[fold_idx + 2]\n\n            # Get date boundaries for test period\n            test_start_date = unique_dates.iloc[test_start_idx]\n            test_end_date = unique_dates.iloc[test_end_idx - 1]\n\n            # Apply purge and embargo using TRADING DAY counts\n            # Purge: exclude training samples whose label window overlaps test\n            purge_cutoff_idx = max(0, test_start_idx - self.max_label_horizon)\n\n            # Embargo: additional safety buffer\n            embargo_cutoff_idx = max(0, test_start_idx - self.embargo_days)\n\n            # Combined cutoff: most restrictive (earliest index)\n            train_cutoff_idx = min(purge_cutoff_idx, embargo_cutoff_idx)\n            train_cutoff_date = (\n                unique_dates.iloc[train_cutoff_idx]\n                if train_cutoff_idx < n_dates\n                else unique_dates.iloc[0]\n            )\n\n            # Find indices for train and test\n            train_mask = (dates >= unique_dates.iloc[0]) & (dates < train_cutoff_date)\n            test_mask = (dates >= test_start_date) & (dates <= test_end_date)\n\n            train_indices = np.where(train_mask)[0]\n            test_indices = np.where(test_mask)[0]\n\n            # Validate fold has non-empty sets\n            if len(train_indices) == 0:\n                raise ValueError(f\"Fold {fold_idx}: Empty training set after embargo/purge\")\n\n            if len(test_indices) == 0:\n                raise ValueError(f\"Fold {fold_idx}: Empty test set\")\n\n            # Log fold info\n            train_max_date = dates[train_indices].max()\n            test_min_date = dates[test_indices].min()\n            gap_mask = (unique_dates > train_max_date) & (unique_dates < test_min_date)\n            trading_day_gap = gap_mask.sum() + 1\n\n            logger.info(\n                f\"Fold {fold_idx}: train={len(train_indices)} samples, \"\n                f\"test={len(test_indices)} samples, gap={trading_day_gap} trading days\"\n            )\n\n            # Validate\n            self._validate_fold(fold_idx, train_indices, test_indices, dates, unique_dates)\n\n            yield (train_indices, test_indices)\n\n    def _validate_fold(\n        self,\n        fold_idx: int,\n        train_idx: np.ndarray,\n        test_idx: np.ndarray,\n        dates: pd.Series,\n        unique_dates: pd.Series,\n    ) -> None:\n        \"\"\"Validate fold satisfies embargo and purge constraints in trading days.\"\"\"\n        if len(train_idx) == 0 or len(test_idx) == 0:\n            raise ValueError(f\"Fold {fold_idx}: Empty train or test set\")\n\n        train_max = dates[train_idx].max()\n        test_min = dates[test_idx].min()\n\n        gap_mask = (unique_dates > train_max) & (unique_dates < test_min)\n        trading_day_gap = gap_mask.sum() + 1\n\n        if trading_day_gap <= self.embargo_days:\n            raise ValueError(\n                f\"Fold {fold_idx}: Embargo violation - \"\n                f\"gap={trading_day_gap} <= embargo={self.embargo_days} trading days\"\n            )\n\n        if trading_day_gap <= self.max_label_horizon:\n            raise ValueError(\n                f\"Fold {fold_idx}: Purge violation - \"\n                f\"gap={trading_day_gap} <= horizon={self.max_label_horizon} trading days\"\n            )\n\n    def get_n_splits(self, X=None, y=None, groups=None) -> int:\n        \"\"\"Return the number of splits (sklearn-compatible).\"\"\"\n        return self.n_splits\n\n\ndef create_cv_from_config(config: dict) -> PurgedEmbargoedTimeSeriesSplit:\n    \"\"\"Create splitter from configuration dictionary.\"\"\"\n    cv_config = config.get(\"cv_scheme\", {})\n    labels_config = config.get(\"labels\", {})\n\n    n_splits = cv_config.get(\"n_splits\", 5)\n\n    if \"embargo_days\" in cv_config:\n        config_embargo = cv_config[\"embargo_days\"]\n        if config_embargo != EMBARGO_DAYS:\n            raise ValueError(\n                f\"embargo_days in config ({config_embargo}) must be {EMBARGO_DAYS} (NON-NEGOTIABLE)\"\n            )\n\n    horizons = labels_config.get(\"horizons\", [21, 63])\n    max_label_horizon = max(horizons)\n\n    return PurgedEmbargoedTimeSeriesSplit(\n        n_splits=n_splits,\n        max_label_horizon=max_label_horizon,\n    )\n\n\n# ============================================================================\n# Chunk 4: Base Model Training Stubs\n# ============================================================================\n# NOTE: sklearn.linear_model.Ridge and xgboost will be imported during implementation\n\n\nclass BaseModelTrainer(ABC):\n    \"\"\"\n    Abstract base class for model trainers.\n\n    Provides consistent interface for Ridge, XGBoost, and future models.\n    Per P3C4-001-001: define abstract methods for training and prediction.\n\n    Methods:\n        fit(X, y): Train the model on features X and labels y\n        predict(X): Generate predictions for features X\n        get_feature_importance(): Return feature importance (or None)\n        get_params(): Return model hyperparameters\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"BaseModelTrainer\":\n        \"\"\"Train the model.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n            y: Target labels (N_samples,)\n\n        Returns:\n            Self for chaining\n\n        Raises:\n            ValueError: If X or y are empty or contain invalid values\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate predictions.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n\n        Returns:\n            Array of predictions (N_samples,)\n\n        Raises:\n            RuntimeError: If model not fitted\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract feature importance.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Returns None if model does not support feature importance\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return model hyperparameters.\n\n        Returns:\n            Dictionary of hyperparameters\n        \"\"\"\n        raise NotImplementedError\n\n\nclass RidgeTrainer(BaseModelTrainer):\n    \"\"\"\n    Ridge regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-002: alpha=3.0, random_state=42 (NON-NEGOTIABLE)\n\n    \"\"\"\n\n    ALPHA: float = 3.0\n    RANDOM_STATE: int = 42\n\n    def __init__(self, alpha: float = ALPHA, random_state: int = RANDOM_STATE):\n        \"\"\"Initialize Ridge trainer with frozen hyperparameters.\"\"\"\n        if not math.isclose(alpha, self.ALPHA, abs_tol=1e-12):\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter alpha is frozen at {self.ALPHA} (got {alpha}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = Ridge(alpha=self.ALPHA, random_state=self.RANDOM_STATE)\n        self._is_fitted = False\n\n        logger.debug(\n            \"Initialized RidgeTrainer with alpha=%s, random_state=%s (frozen values).\",\n            self.ALPHA,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"RidgeTrainer\":\n        \"\"\"Train Ridge model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in X or y: sklearn raises, propagate\n        - Singular matrix: Ridge regularization prevents this\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        self.model.fit(X, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted RidgeTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate Ridge predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"RidgeTrainer must be fitted before calling predict().\")\n\n        predictions = self.model.predict(X)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Ridge does not have feature importance.\n\n        Returns:\n            None (Ridge coefficients exist but not standardized as 'importance')\n        \"\"\"\n        return None\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return Ridge hyperparameters.\"\"\"\n        return {\n            \"alpha\": self.ALPHA,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\nclass XGBoostTrainer(BaseModelTrainer):\n    \"\"\"\n    XGBoost regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-003 and specs:\n    - max_depth=6\n    - n_estimators=400\n    - learning_rate=0.05 (eta)\n    - subsample=0.8\n    - colsample_bytree=0.8\n    - random_state=42\n    All parameters NON-NEGOTIABLE per specs Section 1.\n\n    Uses xgboost.XGBRegressor with tree_method='hist' for memory efficiency.\n    Sanitizes feature names by replacing special characters with underscores.\n    \"\"\"\n\n    MAX_DEPTH: int = 6\n    N_ESTIMATORS: int = 400\n    LEARNING_RATE: float = 0.05\n    SUBSAMPLE: float = 0.8\n    COLSAMPLE_BYTREE: float = 0.8\n    RANDOM_STATE: int = 42\n\n    def __init__(\n        self,\n        max_depth: int = 6,\n        n_estimators: int = 400,\n        learning_rate: float = 0.05,\n        subsample: float = 0.8,\n        colsample_bytree: float = 0.8,\n        random_state: int = 42,\n    ):\n        \"\"\"Initialize XGBoost trainer with frozen hyperparameters.\n\n        Args:\n            max_depth: Maximum tree depth (default 6)\n            n_estimators: Number of boosting rounds (default 400)\n            learning_rate: Step size shrinkage (default 0.05)\n            subsample: Subsample ratio of training instances (default 0.8)\n            colsample_bytree: Subsample ratio of features (default 0.8)\n            random_state: Random seed (default 42)\n\n        Raises:\n            ValueError: If any hyperparameter does not match frozen value\n        \"\"\"\n        import xgboost as xgb\n\n        # Validate frozen hyperparameters\n        if max_depth != self.MAX_DEPTH:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter max_depth is frozen at {self.MAX_DEPTH} (got {max_depth}).\"\n            )\n        if n_estimators != self.N_ESTIMATORS:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter n_estimators is frozen at {self.N_ESTIMATORS} (got {n_estimators}).\"\n            )\n        if not math.isclose(learning_rate, self.LEARNING_RATE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter learning_rate is frozen at {self.LEARNING_RATE} (got {learning_rate}).\"\n            )\n        if not math.isclose(subsample, self.SUBSAMPLE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter subsample is frozen at {self.SUBSAMPLE} (got {subsample}).\"\n            )\n        if not math.isclose(colsample_bytree, self.COLSAMPLE_BYTREE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter colsample_bytree is frozen at {self.COLSAMPLE_BYTREE} (got {colsample_bytree}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = xgb.XGBRegressor(\n            max_depth=self.MAX_DEPTH,\n            n_estimators=self.N_ESTIMATORS,\n            learning_rate=self.LEARNING_RATE,\n            subsample=self.SUBSAMPLE,\n            colsample_bytree=self.COLSAMPLE_BYTREE,\n            random_state=self.RANDOM_STATE,\n            tree_method=\"hist\",\n        )\n        self._is_fitted = False\n        self._feature_name_mapping: dict[str, str] = {}\n        self._sanitized_feature_order: list[str] = []\n\n        logger.debug(\n            \"Initialized XGBoostTrainer with max_depth=%s, n_estimators=%s, learning_rate=%s, \"\n            \"subsample=%s, colsample_bytree=%s, random_state=%s (frozen values).\",\n            self.MAX_DEPTH,\n            self.N_ESTIMATORS,\n            self.LEARNING_RATE,\n            self.SUBSAMPLE,\n            self.COLSAMPLE_BYTREE,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"XGBoostTrainer\":\n        \"\"\"Train XGBoost model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in y: XGBoost raises, propagate\n        - Feature names with special chars: Sanitize before training\n        - Duplicate sanitized names: Raise ValueError with collision details\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        sanitized_columns = []\n        sanitized_to_original: dict[str, list[str]] = {}\n\n        for col in X.columns:\n            original = str(col)\n            sanitized = re.sub(r\"[^A-Za-z0-9_]\", \"_\", original)\n            sanitized_columns.append(sanitized)\n\n            if sanitized not in sanitized_to_original:\n                sanitized_to_original[sanitized] = []\n            sanitized_to_original[sanitized].append(original)\n\n        duplicates = {\n            san: orig_list for san, orig_list in sanitized_to_original.items() if len(orig_list) > 1\n        }\n        if duplicates:\n            collision_info = []\n            for san, orig_list in list(duplicates.items())[:3]:\n                collision_info.append(f\"{orig_list} -> '{san}'\")\n            raise ValueError(\n                f\"Feature name sanitization produced {len(duplicates)} duplicate(s). \"\n                f\"XGBoost requires unique feature names. Collisions: {'; '.join(collision_info)}\"\n            )\n\n        self._feature_name_mapping = {\n            san: orig_list[0] for san, orig_list in sanitized_to_original.items()\n        }\n        self._sanitized_feature_order = sanitized_columns.copy()\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        self.model.fit(X_sanitized, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted XGBoostTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate XGBoost predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"XGBoostTrainer must be fitted before calling predict().\")\n\n        sanitized_columns = []\n        for col in X.columns:\n            sanitized_columns.append(re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(col)))\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        if self._sanitized_feature_order:\n            missing_features = set(self._sanitized_feature_order) - set(X_sanitized.columns)\n            unexpected_features = set(X_sanitized.columns) - set(self._sanitized_feature_order)\n            if missing_features or unexpected_features:\n                raise ValueError(\n                    \"Prediction features must match training features exactly. \"\n                    f\"Missing: {sorted(missing_features)}; Unexpected: {sorted(unexpected_features)}\"\n                )\n            X_sanitized = X_sanitized[self._sanitized_feature_order]\n\n        predictions = self.model.predict(X_sanitized)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract XGBoost feature importance.\n\n        Per P3C4-001-007: extract gain and weight importance.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Sorted by importance_gain descending\n        \"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\n                \"XGBoostTrainer must be fitted before extracting feature importance.\"\n            )\n\n        booster = self.model.get_booster()\n\n        gain_scores = booster.get_score(importance_type=\"gain\")\n        weight_scores = booster.get_score(importance_type=\"weight\")\n\n        if not gain_scores and not weight_scores:\n            return pd.DataFrame(columns=[\"feature\", \"importance_gain\", \"importance_weight\"])\n\n        all_features = set(gain_scores.keys()) | set(weight_scores.keys())\n        rows = []\n        for sanitized_feature in all_features:\n            original_name = self._feature_name_mapping.get(sanitized_feature, sanitized_feature)\n            rows.append(\n                {\n                    \"feature\": original_name,\n                    \"importance_gain\": gain_scores.get(sanitized_feature, 0.0),\n                    \"importance_weight\": weight_scores.get(sanitized_feature, 0.0),\n                }\n            )\n\n        df = pd.DataFrame(rows)\n        df = df.sort_values(\"importance_gain\", ascending=False).reset_index(drop=True)\n\n        return df\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return XGBoost hyperparameters.\"\"\"\n        return {\n            \"max_depth\": self.MAX_DEPTH,\n            \"n_estimators\": self.N_ESTIMATORS,\n            \"learning_rate\": self.LEARNING_RATE,\n            \"subsample\": self.SUBSAMPLE,\n            \"colsample_bytree\": self.COLSAMPLE_BYTREE,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\ndef run_cv_training(\n    X: pd.DataFrame,\n    y: pd.Series,\n    cv_splitter: PurgedEmbargoedTimeSeriesSplit,\n    trainer: BaseModelTrainer,\n    model_name: str,\n    horizon: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Orchestrate cross-validation training loop for a single model-horizon pair.\n\n    Per P3C4-001-006: train on each CV fold, collect OOF predictions,\n    compute CV scores, train final model on all data.\n\n    Args:\n        X: Feature matrix with MultiIndex (instrument, datetime)\n        y: Target labels with MultiIndex (instrument, datetime)\n        cv_splitter: CPCV splitter from Chunk 3\n        trainer: BaseModelTrainer instance (RidgeTrainer or XGBoostTrainer)\n        model_name: Model identifier (\"ridge\" or \"xgboost\")\n        horizon: Horizon identifier (\"21d\" or \"63d\")\n\n    Returns:\n        Dictionary with keys:\n        - 'oof_predictions': DataFrame with columns [prediction, fold_id]\n        - 'final_model': Trained BaseModelTrainer on full data\n        - 'cv_scores': List of dicts with per-fold metrics\n\n    Raises:\n        ValueError: If CV fold has < 100 samples or training fails\n\n    Edge cases:\n        - Small fold: Validate min 100 samples per fold\n        - Training failure: Log error with fold context, re-raise\n        - Outlier predictions: Log warning if >1% exceed ±500 bps\n    \"\"\"\n    # TODO: Initialize OOF prediction storage\n    # TODO: Initialize CV scores list\n    # TODO: For each fold from cv_splitter.split(X):\n    #   - Extract train/test indices\n    #   - Validate fold size >= 100\n    #   - Clone trainer for fold\n    #   - Fit on train data\n    #   - Predict on test data\n    #   - Store predictions with fold_id\n    #   - Compute and log fold metrics (r2, mse, mae)\n    # TODO: Aggregate OOF predictions (call aggregate_oof_predictions)\n    # TODO: Train final model on all data\n    # TODO: Return results dict\n    raise NotImplementedError(\"P3C4-001-006: run_cv_training implementation pending\")\n\n\ndef aggregate_oof_predictions(\n    fold_predictions: Iterable[tuple[Sequence[Any], Sequence[float], int]],\n) -> pd.DataFrame:\n    \"\"\"Aggregate out-of-fold predictions from individual cross-validation folds.\n\n    Args:\n        fold_predictions: Iterable of tuples ``(indices, predictions, fold_id)`` where\n            ``indices`` is a 1-D sequence of sample indices,\n            ``predictions`` is a 1-D sequence aligned with ``indices``,\n            and ``fold_id`` identifies the fold.\n\n    Returns:\n        DataFrame indexed by the provided ``indices`` with columns ``prediction`` and\n        ``fold_id``.\n\n    Raises:\n        ValueError: If no folds are provided, shapes mismatch, indices overlap, or\n            predictions contain NaN/Inf values.\n    \"\"\"\n    entries = list(fold_predictions)\n    if not entries:\n        raise ValueError(\"fold_predictions cannot be empty.\")\n\n    frames: list[pd.DataFrame] = []\n    seen_indices: set[Any] = set()\n\n    for entry in entries:\n        if len(entry) != 3:\n            raise ValueError(\n                \"Each fold prediction entry must be a tuple of (indices, predictions, fold_id).\"\n            )\n\n        indices, predictions, fold_id = entry\n        index = pd.Index(indices)\n        preds_array = np.asarray(predictions, dtype=np.float32)\n\n        if preds_array.ndim != 1:\n            raise ValueError(f\"Predictions for fold {fold_id} must be one-dimensional.\")\n        if len(index) != len(preds_array):\n            raise ValueError(\n                f\"Fold {fold_id} has mismatched indices ({len(index)}) and predictions \"\n                f\"({len(preds_array)}).\"\n            )\n        if not np.isfinite(preds_array).all():\n            raise ValueError(f\"Fold {fold_id} contains NaN or infinite predictions.\")\n\n        index_list = index.tolist()\n        duplicate_indices = set(index_list) & seen_indices\n        if duplicate_indices:\n            duplicates_preview = list(duplicate_indices)[:3]\n            raise ValueError(\n                f\"Detected overlapping OOF indices between folds: {duplicates_preview}\"\n            )\n\n        seen_indices.update(index_list)\n\n        fold_frame = pd.DataFrame({\"prediction\": preds_array}, index=index)\n        fold_frame[\"fold_id\"] = np.int8(fold_id)\n        frames.append(fold_frame)\n\n        logger.debug(\n            \"Aggregated %s predictions for fold %s.\",\n            len(fold_frame),\n            fold_id,\n        )\n\n    aggregated = pd.concat(frames).sort_index()\n    aggregated = aggregated[[\"prediction\", \"fold_id\"]]\n    return aggregated\n"
    }
  ],
  "tests": [
    {
      "path": "tests/unit/test_model2_oof_aggregation.py",
      "content": "\"\"\"Unit tests for OOF prediction aggregation (P3C4-001-005).\n\nTests the aggregate_oof_predictions function which combines\nout-of-fold predictions from multiple cross-validation folds.\n\nPer ticket P3C4-001-005:\n- Full coverage verification (all indices present once)\n- Duplicate detection (overlapping indices)\n- NaN/Inf prediction detection\n- Empty fold handling\n- Shape mismatch detection\n- Invalid input format handling\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom src.model2.train import aggregate_oof_predictions\n\n\ndef test_oof_aggregation_full_coverage():\n    \"\"\"Verify all indices present exactly once across 5 folds.\n\n    Per P3C4-001-005: test_oof_aggregation_full_coverage requirement.\n    Create 5 non-overlapping folds and verify aggregation produces\n    complete coverage with correct fold_id assignment.\n    \"\"\"\n    # Create 5 non-overlapping folds with 20 samples each\n    fold_predictions = [\n        (np.arange(0, 20), np.random.randn(20), 0),\n        (np.arange(20, 40), np.random.randn(20), 1),\n        (np.arange(40, 60), np.random.randn(20), 2),\n        (np.arange(60, 80), np.random.randn(20), 3),\n        (np.arange(80, 100), np.random.randn(20), 4),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify result is a DataFrame\n    assert isinstance(result, pd.DataFrame)\n\n    # Verify all indices present\n    assert len(result) == 100\n    assert set(result.index) == set(range(100))\n\n    # Verify columns\n    assert list(result.columns) == [\"prediction\", \"fold_id\"]\n\n    # Verify fold_id assignment\n    for idx in range(0, 20):\n        assert result.loc[idx, \"fold_id\"] == 0\n    for idx in range(20, 40):\n        assert result.loc[idx, \"fold_id\"] == 1\n    for idx in range(40, 60):\n        assert result.loc[idx, \"fold_id\"] == 2\n    for idx in range(60, 80):\n        assert result.loc[idx, \"fold_id\"] == 3\n    for idx in range(80, 100):\n        assert result.loc[idx, \"fold_id\"] == 4\n\n    # Verify predictions are finite\n    assert np.all(np.isfinite(result[\"prediction\"]))\n\n    # Verify result is sorted by index\n    assert result.index.is_monotonic_increasing\n\n\ndef test_oof_aggregation_duplicate_indices():\n    \"\"\"Verify ValueError raised on overlapping fold indices.\n\n    Per P3C4-001-005: test_oof_aggregation_duplicate_indices requirement.\n    Create 2 folds with overlapping indices and verify error message\n    contains overlap information.\n    \"\"\"\n    # Create 2 folds with overlapping indices (5, 6, 7)\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10), 0),\n        (np.arange(5, 15), np.random.randn(10), 1),  # Overlap: 5, 6, 7, 8, 9\n    ]\n\n    with pytest.raises(ValueError, match=\"overlapping OOF indices\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_nan_prediction():\n    \"\"\"Verify ValueError raised on NaN predictions.\n\n    Per P3C4-001-005: test_oof_aggregation_nan_prediction requirement.\n    Create fold with NaN prediction and verify error message contains\n    fold_id information.\n    \"\"\"\n    # Create fold with NaN prediction\n    predictions_with_nan = np.array([1.0, 2.0, np.nan, 4.0, 5.0])\n    fold_predictions = [\n        (np.arange(0, 5), predictions_with_nan, 0),\n    ]\n\n    with pytest.raises(ValueError, match=\"Fold 0 contains NaN or infinite predictions\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_inf_prediction():\n    \"\"\"Verify ValueError raised on infinite predictions.\n\n    Edge case: Predictions contain positive or negative infinity.\n    \"\"\"\n    # Create fold with positive infinity\n    predictions_with_inf = np.array([1.0, 2.0, np.inf, 4.0, 5.0])\n    fold_predictions = [\n        (np.arange(0, 5), predictions_with_inf, 0),\n    ]\n\n    with pytest.raises(ValueError, match=\"Fold 0 contains NaN or infinite predictions\"):\n        aggregate_oof_predictions(fold_predictions)\n\n    # Create fold with negative infinity\n    predictions_with_neg_inf = np.array([1.0, 2.0, -np.inf, 4.0, 5.0])\n    fold_predictions = [\n        (np.arange(0, 5), predictions_with_neg_inf, 1),\n    ]\n\n    with pytest.raises(ValueError, match=\"Fold 1 contains NaN or infinite predictions\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_empty_fold_list():\n    \"\"\"Verify ValueError raised on empty fold list.\n\n    Edge case per implementation: empty fold_predictions should raise ValueError.\n    \"\"\"\n    fold_predictions = []\n\n    with pytest.raises(ValueError, match=\"fold_predictions cannot be empty\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_shape_mismatch():\n    \"\"\"Verify ValueError raised when indices and predictions have different lengths.\n\n    Edge case: Shape mismatch between indices and predictions arrays.\n    \"\"\"\n    # Indices: 10 elements, Predictions: 5 elements\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(5), 0),\n    ]\n\n    with pytest.raises(\n        ValueError, match=\"Fold 0 has mismatched indices \\\\(10\\\\) and predictions \\\\(5\\\\)\"\n    ):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_invalid_tuple_format():\n    \"\"\"Verify ValueError raised on invalid tuple format.\n\n    Edge case: Each entry must be a 3-tuple (indices, predictions, fold_id).\n    \"\"\"\n    # Only 2 elements in tuple (missing fold_id)\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10)),  # type: ignore\n    ]\n\n    with pytest.raises(\n        ValueError,\n        match=\"Each fold prediction entry must be a tuple of \\\\(indices, predictions, fold_id\\\\)\",\n    ):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_non_1d_predictions():\n    \"\"\"Verify ValueError raised when predictions are not 1-dimensional.\n\n    Edge case: Predictions must be 1-D array.\n    \"\"\"\n    # 2-D predictions array\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10, 2), 0),  # 2-D instead of 1-D\n    ]\n\n    with pytest.raises(ValueError, match=\"Predictions for fold 0 must be one-dimensional\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_string_indices():\n    \"\"\"Verify aggregation works with string indices (e.g., instrument IDs).\n\n    Per implementation: indices can be any hashable type, not just integers.\n    This is important for MultiIndex scenarios with (instrument, datetime).\n    \"\"\"\n    # Use string indices instead of integers\n    fold_predictions = [\n        ([\"AAPL\", \"GOOGL\", \"MSFT\"], np.array([0.1, 0.2, 0.3]), 0),\n        ([\"TSLA\", \"AMZN\"], np.array([0.4, 0.5]), 1),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify all indices present\n    assert len(result) == 5\n    assert set(result.index) == {\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\", \"AMZN\"}\n\n    # Verify fold_id assignment\n    assert result.loc[\"AAPL\", \"fold_id\"] == 0\n    assert result.loc[\"GOOGL\", \"fold_id\"] == 0\n    assert result.loc[\"MSFT\", \"fold_id\"] == 0\n    assert result.loc[\"TSLA\", \"fold_id\"] == 1\n    assert result.loc[\"AMZN\", \"fold_id\"] == 1\n\n\ndef test_oof_aggregation_tuple_indices():\n    \"\"\"Verify aggregation works with tuple indices (e.g., MultiIndex-like).\n\n    Edge case: indices can be tuples representing (instrument, datetime) pairs.\n    \"\"\"\n    # Use tuple indices\n    fold_predictions = [\n        ([(\"AAPL\", \"2020-01-01\"), (\"AAPL\", \"2020-01-02\")], np.array([0.1, 0.2]), 0),\n        ([(\"GOOGL\", \"2020-01-01\"), (\"GOOGL\", \"2020-01-02\")], np.array([0.3, 0.4]), 1),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify all indices present\n    assert len(result) == 4\n    expected_indices = {\n        (\"AAPL\", \"2020-01-01\"),\n        (\"AAPL\", \"2020-01-02\"),\n        (\"GOOGL\", \"2020-01-01\"),\n        (\"GOOGL\", \"2020-01-02\"),\n    }\n    assert set(result.index) == expected_indices\n\n\ndef test_oof_aggregation_single_fold():\n    \"\"\"Verify aggregation works with single fold.\n\n    Edge case: Only one fold provided (valid for single split).\n    \"\"\"\n    fold_predictions = [\n        (np.arange(0, 50), np.random.randn(50), 0),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify single fold aggregated correctly\n    assert len(result) == 50\n    assert np.all(result[\"fold_id\"] == 0)\n    assert list(result.columns) == [\"prediction\", \"fold_id\"]\n\n\ndef test_oof_aggregation_preserves_prediction_values():\n    \"\"\"Verify prediction values are preserved during aggregation.\n\n    Test that input prediction values match output prediction values exactly.\n    \"\"\"\n    # Create known prediction values\n    np.random.seed(42)\n    predictions_fold0 = np.random.randn(10)\n    predictions_fold1 = np.random.randn(10)\n\n    fold_predictions = [\n        (np.arange(0, 10), predictions_fold0, 0),\n        (np.arange(10, 20), predictions_fold1, 1),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify prediction values match\n    for idx in range(10):\n        assert np.isclose(result.loc[idx, \"prediction\"], predictions_fold0[idx])\n    for idx in range(10, 20):\n        assert np.isclose(result.loc[idx, \"prediction\"], predictions_fold1[idx - 10])\n\n\ndef test_oof_aggregation_fold_id_type():\n    \"\"\"Verify fold_id column has correct dtype (int8).\n\n    Per implementation: fold_id is stored as np.int8 for memory efficiency.\n    \"\"\"\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10), 0),\n        (np.arange(10, 20), np.random.randn(10), 1),  # Fixed: 10 predictions, not 20\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify fold_id dtype is int8\n    assert result[\"fold_id\"].dtype == np.int8\n\n\ndef test_oof_aggregation_large_fold_ids():\n    \"\"\"Verify aggregation works with larger fold IDs.\n\n    Edge case: fold_id can be any integer, not limited to 0-4.\n    \"\"\"\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10), 10),\n        (np.arange(10, 20), np.random.randn(10), 20),\n        (np.arange(20, 30), np.random.randn(10), 30),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify fold_id values preserved\n    assert set(result[\"fold_id\"].unique()) == {10, 20, 30}\n    assert np.all(result.loc[0:9, \"fold_id\"] == 10)\n    assert np.all(result.loc[10:19, \"fold_id\"] == 20)\n    assert np.all(result.loc[20:29, \"fold_id\"] == 30)\n\n\ndef test_oof_aggregation_duplicate_detection_multiple_folds():\n    \"\"\"Verify duplicate detection works across multiple folds.\n\n    Edge case: Duplicates between non-adjacent folds should be detected.\n    \"\"\"\n    # Fold 0 and Fold 2 have overlapping indices (not adjacent)\n    fold_predictions = [\n        (np.arange(0, 10), np.random.randn(10), 0),\n        (np.arange(10, 20), np.random.randn(10), 1),\n        (np.arange(5, 15), np.random.randn(10), 2),  # Overlaps with both fold 0 and 1\n    ]\n\n    with pytest.raises(ValueError, match=\"overlapping OOF indices\"):\n        aggregate_oof_predictions(fold_predictions)\n\n\ndef test_oof_aggregation_predictions_as_list():\n    \"\"\"Verify aggregation works when predictions are provided as lists.\n\n    Per implementation: predictions are converted to np.ndarray via np.asarray.\n    \"\"\"\n    fold_predictions = [\n        (list(range(0, 10)), [0.1] * 10, 0),  # Lists instead of arrays\n        (list(range(10, 20)), [0.2] * 10, 1),\n    ]\n\n    result = aggregate_oof_predictions(fold_predictions)\n\n    # Verify aggregation succeeded\n    assert len(result) == 20\n    assert list(result.columns) == [\"prediction\", \"fold_id\"]\n\n    # Verify prediction values\n    assert np.allclose(result.loc[0:9, \"prediction\"], 0.1)\n    assert np.allclose(result.loc[10:19, \"prediction\"], 0.2)\n"
    }
  ]
}
