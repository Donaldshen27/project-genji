{
  "ticket": "P3C4-001-009",
  "files": [
    {
      "path": "src/model2/base_models.py",
      "content": "\"\"\"Model 2 Base Models: Trainers and CV Orchestration.\n\nImplements BaseModelTrainer ABC, RidgeTrainer, XGBoostTrainer,\nand CV training loop orchestrator.\n\nThis module is extracted from src/model2/train.py to separate\nbase model implementations from the CPCV splitting logic.\n\"\"\"\n\nimport logging\nimport math\nimport re\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Iterable, Sequence\nfrom pathlib import Path\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseModelTrainer(ABC):\n    \"\"\"\n    Abstract base class for model trainers.\n\n    Provides consistent interface for Ridge, XGBoost, and future models.\n    Per P3C4-001-001: define abstract methods for training and prediction.\n\n    Methods:\n        fit(X, y): Train the model on features X and labels y\n        predict(X): Generate predictions for features X\n        get_feature_importance(): Return feature importance (or None)\n        get_params(): Return model hyperparameters\n    \"\"\"\n\n    @abstractmethod\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"BaseModelTrainer\":\n        \"\"\"Train the model.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n            y: Target labels (N_samples,)\n\n        Returns:\n            Self for chaining\n\n        Raises:\n            ValueError: If X or y are empty or contain invalid values\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate predictions.\n\n        Args:\n            X: Feature matrix (N_samples, N_features)\n\n        Returns:\n            Array of predictions (N_samples,)\n\n        Raises:\n            RuntimeError: If model not fitted\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract feature importance.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Returns None if model does not support feature importance\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return model hyperparameters.\n\n        Returns:\n            Dictionary of hyperparameters\n        \"\"\"\n        raise NotImplementedError\n\n\nclass RidgeTrainer(BaseModelTrainer):\n    \"\"\"\n    Ridge regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-002: alpha=3.0, random_state=42 (NON-NEGOTIABLE)\n\n    \"\"\"\n\n    ALPHA: float = 3.0\n    RANDOM_STATE: int = 42\n\n    def __init__(self, alpha: float = ALPHA, random_state: int = RANDOM_STATE):\n        \"\"\"Initialize Ridge trainer with frozen hyperparameters.\"\"\"\n        if not math.isclose(alpha, self.ALPHA, abs_tol=1e-12):\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter alpha is frozen at {self.ALPHA} (got {alpha}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"RidgeTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = Ridge(alpha=self.ALPHA, random_state=self.RANDOM_STATE)\n        self._is_fitted = False\n\n        logger.debug(\n            \"Initialized RidgeTrainer with alpha=%s, random_state=%s (frozen values).\",\n            self.ALPHA,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"RidgeTrainer\":\n        \"\"\"Train Ridge model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in X or y: sklearn raises, propagate\n        - Singular matrix: Ridge regularization prevents this\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit Ridge model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        self.model.fit(X, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted RidgeTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate Ridge predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"RidgeTrainer must be fitted before calling predict().\")\n\n        predictions = self.model.predict(X)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Ridge does not have feature importance.\n\n        Returns:\n            None (Ridge coefficients exist but not standardized as 'importance')\n        \"\"\"\n        return None\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return Ridge hyperparameters.\"\"\"\n        return {\n            \"alpha\": self.ALPHA,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\nclass XGBoostTrainer(BaseModelTrainer):\n    \"\"\"\n    XGBoost regression trainer with frozen hyperparameters.\n\n    Per P3C4-001-003 and specs:\n    - max_depth=6\n    - n_estimators=400\n    - learning_rate=0.05 (eta)\n    - subsample=0.8\n    - colsample_bytree=0.8\n    - random_state=42\n    All parameters NON-NEGOTIABLE per specs Section 1.\n\n    Uses xgboost.XGBRegressor with tree_method='hist' for memory efficiency.\n    Sanitizes feature names by replacing special characters with underscores.\n    \"\"\"\n\n    MAX_DEPTH: int = 6\n    N_ESTIMATORS: int = 400\n    LEARNING_RATE: float = 0.05\n    SUBSAMPLE: float = 0.8\n    COLSAMPLE_BYTREE: float = 0.8\n    RANDOM_STATE: int = 42\n\n    def __init__(\n        self,\n        max_depth: int = 6,\n        n_estimators: int = 400,\n        learning_rate: float = 0.05,\n        subsample: float = 0.8,\n        colsample_bytree: float = 0.8,\n        random_state: int = 42,\n    ):\n        \"\"\"Initialize XGBoost trainer with frozen hyperparameters.\n\n        Args:\n            max_depth: Maximum tree depth (default 6)\n            n_estimators: Number of boosting rounds (default 400)\n            learning_rate: Step size shrinkage (default 0.05)\n            subsample: Subsample ratio of training instances (default 0.8)\n            colsample_bytree: Subsample ratio of features (default 0.8)\n            random_state: Random seed (default 42)\n\n        Raises:\n            ValueError: If any hyperparameter does not match frozen value\n        \"\"\"\n        import xgboost as xgb\n\n        # Validate frozen hyperparameters\n        if max_depth != self.MAX_DEPTH:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter max_depth is frozen at {self.MAX_DEPTH} (got {max_depth}).\"\n            )\n        if n_estimators != self.N_ESTIMATORS:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter n_estimators is frozen at {self.N_ESTIMATORS} (got {n_estimators}).\"\n            )\n        if not math.isclose(learning_rate, self.LEARNING_RATE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter learning_rate is frozen at {self.LEARNING_RATE} (got {learning_rate}).\"\n            )\n        if not math.isclose(subsample, self.SUBSAMPLE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter subsample is frozen at {self.SUBSAMPLE} (got {subsample}).\"\n            )\n        if not math.isclose(colsample_bytree, self.COLSAMPLE_BYTREE, abs_tol=1e-12):\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter colsample_bytree is frozen at {self.COLSAMPLE_BYTREE} (got {colsample_bytree}).\"\n            )\n        if random_state != self.RANDOM_STATE:\n            raise ValueError(\n                f\"XGBoostTrainer hyperparameter random_state is frozen at {self.RANDOM_STATE} (got {random_state}).\"\n            )\n\n        self.model = xgb.XGBRegressor(\n            max_depth=self.MAX_DEPTH,\n            n_estimators=self.N_ESTIMATORS,\n            learning_rate=self.LEARNING_RATE,\n            subsample=self.SUBSAMPLE,\n            colsample_bytree=self.COLSAMPLE_BYTREE,\n            random_state=self.RANDOM_STATE,\n            tree_method=\"hist\",\n        )\n        self._is_fitted = False\n        self._feature_name_mapping: dict[str, str] = {}\n        self._sanitized_feature_order: list[str] = []\n\n        logger.debug(\n            \"Initialized XGBoostTrainer with max_depth=%s, n_estimators=%s, learning_rate=%s, \"\n            \"subsample=%s, colsample_bytree=%s, random_state=%s (frozen values).\",\n            self.MAX_DEPTH,\n            self.N_ESTIMATORS,\n            self.LEARNING_RATE,\n            self.SUBSAMPLE,\n            self.COLSAMPLE_BYTREE,\n            self.RANDOM_STATE,\n        )\n\n    def fit(self, X: pd.DataFrame, y: pd.Series) -> \"XGBoostTrainer\":\n        \"\"\"Train XGBoost model.\n\n        Edge cases:\n        - Empty training set: Raise ValueError\n        - NaN in y: XGBoost raises, propagate\n        - Feature names with special chars: Sanitize before training\n        - Duplicate sanitized names: Raise ValueError with collision details\n        \"\"\"\n        if X.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty training set.\")\n        if y.empty:\n            raise ValueError(\"Cannot fit XGBoost model on empty target values.\")\n        if len(X) != len(y):\n            raise ValueError(\n                f\"Feature matrix and target vector must have matching lengths (X={len(X)}, y={len(y)}).\"\n            )\n\n        sanitized_columns = []\n        sanitized_to_original: dict[str, list[str]] = {}\n\n        for col in X.columns:\n            original = str(col)\n            sanitized = re.sub(r\"[^A-Za-z0-9_]\", \"_\", original)\n            sanitized_columns.append(sanitized)\n\n            if sanitized not in sanitized_to_original:\n                sanitized_to_original[sanitized] = []\n            sanitized_to_original[sanitized].append(original)\n\n        duplicates = {\n            san: orig_list for san, orig_list in sanitized_to_original.items() if len(orig_list) > 1\n        }\n        if duplicates:\n            collision_info = []\n            for san, orig_list in list(duplicates.items())[:3]:\n                collision_info.append(f\"{orig_list} -> '{san}'\")\n            raise ValueError(\n                f\"Feature name sanitization produced {len(duplicates)} duplicate(s). \"\n                f\"XGBoost requires unique feature names. Collisions: {'; '.join(collision_info)}\"\n            )\n\n        self._feature_name_mapping = {\n            san: orig_list[0] for san, orig_list in sanitized_to_original.items()\n        }\n        self._sanitized_feature_order = sanitized_columns.copy()\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        self.model.fit(X_sanitized, y)\n        self._is_fitted = True\n\n        logger.debug(\n            \"Fitted XGBoostTrainer on %s samples and %s features.\",\n            X.shape[0],\n            X.shape[1],\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -> np.ndarray:\n        \"\"\"Generate XGBoost predictions.\"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\"XGBoostTrainer must be fitted before calling predict().\")\n\n        sanitized_columns = []\n        for col in X.columns:\n            sanitized_columns.append(re.sub(r\"[^A-Za-z0-9_]\", \"_\", str(col)))\n\n        X_sanitized = X.copy()\n        X_sanitized.columns = sanitized_columns\n\n        if self._sanitized_feature_order:\n            missing_features = set(self._sanitized_feature_order) - set(X_sanitized.columns)\n            unexpected_features = set(X_sanitized.columns) - set(self._sanitized_feature_order)\n            if missing_features or unexpected_features:\n                raise ValueError(\n                    \"Prediction features must match training features exactly. \"\n                    f\"Missing: {sorted(missing_features)}; Unexpected: {sorted(unexpected_features)}\"\n                )\n            X_sanitized = X_sanitized[self._sanitized_feature_order]\n\n        predictions = self.model.predict(X_sanitized)\n        return predictions\n\n    def get_feature_importance(self) -> pd.DataFrame | None:\n        \"\"\"Extract XGBoost feature importance.\n\n        Per P3C4-001-007: extract gain and weight importance, log warnings for\n        edge cases (no features, zero importance), and log top 20 features at INFO level.\n\n        Returns:\n            DataFrame with columns [feature, importance_gain, importance_weight]\n            Sorted by importance_gain descending\n\n        Raises:\n            RuntimeError: If model not fitted\n        \"\"\"\n        if not self._is_fitted:\n            raise RuntimeError(\n                \"XGBoostTrainer must be fitted before extracting feature importance.\"\n            )\n\n        booster = self.model.get_booster()\n\n        gain_scores = booster.get_score(importance_type=\"gain\")\n        weight_scores = booster.get_score(importance_type=\"weight\")\n\n        # Edge case: No features used (constant target or model did not use any splits)\n        if not gain_scores and not weight_scores:\n            logger.warning(\n                \"No features used by XGBoost model. This may indicate a constant target \"\n                \"or insufficient training data.\"\n            )\n            return pd.DataFrame(columns=[\"feature\", \"importance_gain\", \"importance_weight\"])\n\n        all_features = set(gain_scores.keys()) | set(weight_scores.keys())\n        rows = []\n        for sanitized_feature in all_features:\n            original_name = self._feature_name_mapping.get(sanitized_feature, sanitized_feature)\n            rows.append(\n                {\n                    \"feature\": original_name,\n                    \"importance_gain\": gain_scores.get(sanitized_feature, 0.0),\n                    \"importance_weight\": weight_scores.get(sanitized_feature, 0.0),\n                }\n            )\n\n        df = pd.DataFrame(rows)\n        df = df.sort_values(\"importance_gain\", ascending=False).reset_index(drop=True)\n\n        # Edge case: All features have zero importance\n        if (df[\"importance_gain\"] == 0.0).all():\n            logger.warning(\n                \"All features have zero importance_gain. This may indicate model underfitting \"\n                \"or improper feature encoding.\"\n            )\n\n        # Log top 20 features at INFO level\n        top_n = min(20, len(df))\n        if top_n > 0:\n            logger.info(f\"Top {top_n} features by importance_gain:\")\n            for idx in range(top_n):\n                row = df.iloc[idx]\n                logger.info(\n                    f\"  {idx + 1}. {row['feature']}: gain={row['importance_gain']:.4f}, \"\n                    f\"weight={row['importance_weight']:.0f}\"\n                )\n\n        return df\n\n    def get_params(self) -> dict[str, Any]:\n        \"\"\"Return XGBoost hyperparameters.\"\"\"\n        return {\n            \"max_depth\": self.MAX_DEPTH,\n            \"n_estimators\": self.N_ESTIMATORS,\n            \"learning_rate\": self.LEARNING_RATE,\n            \"subsample\": self.SUBSAMPLE,\n            \"colsample_bytree\": self.COLSAMPLE_BYTREE,\n            \"random_state\": self.RANDOM_STATE,\n        }\n\n\ndef save_feature_importance(importance_df: pd.DataFrame, output_path: Path) -> None:\n    \"\"\"Save feature importance DataFrame to parquet file.\n\n    Per P3C4-001-007: Helper function to persist feature importance to disk.\n\n    Args:\n        importance_df: DataFrame with columns [feature, importance_gain, importance_weight]\n        output_path: Path to output parquet file\n\n    Raises:\n        ValueError: If DataFrame schema is invalid\n        OSError: If file write fails\n\n    Edge cases:\n        - Output directory doesn't exist: Create parent directories\n        - File already exists: Overwrite with warning\n        - Empty DataFrame: Write empty parquet (valid edge case)\n    \"\"\"\n    # Validate schema\n    required_columns = {\"feature\", \"importance_gain\", \"importance_weight\"}\n    if not required_columns.issubset(importance_df.columns):\n        missing = required_columns - set(importance_df.columns)\n        raise ValueError(\n            f\"Invalid feature importance schema. Missing columns: {missing}. \"\n            f\"Expected columns: {required_columns}\"\n        )\n\n    # Create parent directory if needed\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Warn if overwriting existing file\n    if output_path.exists():\n        logger.warning(f\"Overwriting existing feature importance file: {output_path}\")\n\n    # Write to parquet\n    try:\n        importance_df.to_parquet(output_path, index=False)\n        logger.info(f\"Saved feature importance ({len(importance_df)} features) to {output_path}\")\n    except Exception as e:\n        raise OSError(f\"Failed to write feature importance to {output_path}: {e}\") from e\n\n\n# ============================================================================\n# CV Training Loop Orchestrator (P3C4-001-006)\n# ============================================================================\n\n\ndef run_cv_training(\n    X: pd.DataFrame,\n    y: pd.Series,\n    cv_splitter: Any,  # PurgedEmbargoedTimeSeriesSplit\n    trainer: BaseModelTrainer,\n    model_name: str,\n    horizon: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Orchestrate cross-validation training loop for a single model-horizon pair.\n\n    Per P3C4-001-006: train on each CV fold, collect OOF predictions,\n    compute CV scores, train final model on all data.\n\n    Args:\n        X: Feature matrix with MultiIndex (instrument, datetime)\n        y: Target labels with MultiIndex (instrument, datetime)\n        cv_splitter: CPCV splitter from Chunk 3\n        trainer: BaseModelTrainer instance (RidgeTrainer or XGBoostTrainer)\n        model_name: Model identifier (\"ridge\" or \"xgboost\")\n        horizon: Horizon identifier (\"21d\" or \"63d\")\n\n    Returns:\n        Dictionary with keys:\n        - 'oof_predictions': DataFrame with columns [prediction, fold_id]\n        - 'final_model': Trained BaseModelTrainer on full data\n        - 'cv_scores': List of dicts with per-fold metrics\n\n    Raises:\n        ValueError: If CV fold has < 100 samples or training fails\n\n    Edge cases:\n        - Small fold: Validate min 100 samples per fold\n        - Training failure: Log error with fold context, re-raise\n        - Outlier predictions: Log warning if all predictions exceed ±1000 bps\n    \"\"\"\n    logger.info(\n        f\"Starting CV training: model={model_name}, horizon={horizon}, \"\n        f\"n_samples={len(X)}, n_features={X.shape[1]}, n_splits={cv_splitter.n_splits}\"\n    )\n\n    # Initialize OOF prediction storage\n    fold_predictions_list: list[tuple[Sequence[Any], Sequence[float], int]] = []\n\n    # Initialize CV scores list\n    cv_scores: list[dict[str, Any]] = []\n\n    # Iterate through CV folds\n    for fold_idx, (train_idx, test_idx) in enumerate(cv_splitter.split(X)):\n        logger.info(f\"Fold {fold_idx}: train_size={len(train_idx)}, test_size={len(test_idx)}\")\n\n        # Validate fold size >= 100 samples\n        if len(test_idx) < 100:\n            raise ValueError(\n                f\"Fold {fold_idx} has {len(test_idx)} test samples, minimum 100 required\"\n            )\n\n        # Clone trainer for this fold (fresh model instance)\n        fold_trainer = _clone_trainer(trainer)\n\n        # Extract train/test data\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        # Fit on train data\n        try:\n            fold_trainer.fit(X_train, y_train)\n        except Exception as e:\n            logger.error(f\"Fold {fold_idx} training failed: {e}\")\n            raise ValueError(f\"Fold {fold_idx} training failed\") from e\n\n        # Predict on test data\n        predictions = fold_trainer.predict(X_test)\n\n        # CRITICAL FIX: Store predictions with X_test.index to preserve MultiIndex\n        # This ensures downstream joins work correctly with (instrument, datetime) labels\n        fold_predictions_list.append((X_test.index, predictions, fold_idx))\n\n        # Compute and log fold metrics\n        fold_metrics = _compute_fold_metrics(\n            y_test, predictions, fold_idx, model_name, horizon\n        )\n        cv_scores.append(fold_metrics)\n        logger.info(\n            f\"Fold {fold_idx} metrics: r2={fold_metrics['r2']:.4f}, \"\n            f\"mse={fold_metrics['mse']:.4f}, mae={fold_metrics['mae']:.4f}\"\n        )\n\n        # Check for outlier predictions\n        _check_outlier_predictions(predictions, fold_idx)\n\n    # Aggregate OOF predictions\n    oof_predictions = aggregate_oof_predictions(fold_predictions_list)\n    logger.info(f\"Aggregated {len(oof_predictions)} OOF predictions across all folds\")\n\n    # Train final model on all data\n    logger.info(f\"Training final model on all {len(X)} samples\")\n    final_trainer = _clone_trainer(trainer)\n    try:\n        final_trainer.fit(X, y)\n    except Exception as e:\n        logger.error(f\"Final model training failed: {e}\")\n        raise ValueError(\"Final model training failed\") from e\n\n    # Log aggregate CV scores\n    if cv_scores:\n        r2_scores = [score[\"r2\"] for score in cv_scores]\n        logger.info(\n            f\"CV aggregate: r2_mean={np.mean(r2_scores):.4f}, r2_std={np.std(r2_scores):.4f}\"\n        )\n\n    # Return results\n    return {\n        \"oof_predictions\": oof_predictions,\n        \"final_model\": final_trainer,\n        \"cv_scores\": cv_scores,\n    }\n\n\ndef _clone_trainer(trainer: BaseModelTrainer) -> BaseModelTrainer:\n    \"\"\"\n    Clone a trainer instance to create a fresh model for a new fold.\n\n    Per P3C4-001-006: Each fold should use a fresh model instance.\n    Uses trainer.__class__() to preserve actual class (including subclasses).\n\n    Args:\n        trainer: Original trainer instance\n\n    Returns:\n        New trainer instance of the same class with default hyperparameters\n\n    Note:\n        This preserves subclass semantics, critical for testing with mock\n        trainers like FailingTrainer(RidgeTrainer).\n    \"\"\"\n    return trainer.__class__()\n\n\ndef _compute_fold_metrics(\n    y_true: pd.Series,\n    y_pred: np.ndarray,\n    fold_idx: int,\n    model_name: str,\n    horizon: str,\n) -> dict[str, Any]:\n    \"\"\"\n    Compute CV metrics for a single fold.\n\n    Per P3C4-001-006: Compute r2, mse, mae for each fold.\n\n    Args:\n        y_true: Ground truth labels\n        y_pred: Model predictions\n        fold_idx: Fold index\n        model_name: Model identifier\n        horizon: Horizon identifier\n\n    Returns:\n        Dictionary with fold metrics: {model, horizon, fold_id, r2, mse, mae}\n\n    Raises:\n        ValueError: If y_pred contains NaN values\n    \"\"\"\n    # Validate predictions are finite\n    if not np.all(np.isfinite(y_pred)):\n        raise ValueError(f\"Fold {fold_idx}: predictions contain NaN or Inf values\")\n\n    # Compute metrics\n    r2 = r2_score(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n\n    # Handle edge case: constant target (r2 may be NaN)\n    if np.allclose(y_true, y_true.iloc[0]):\n        logger.warning(f\"Fold {fold_idx}: constant target detected, r2 may be NaN\")\n\n    # Return metrics dict\n    return {\n        \"model\": model_name,\n        \"horizon\": horizon,\n        \"fold_id\": fold_idx,\n        \"r2\": float(r2),\n        \"mse\": float(mse),\n        \"mae\": float(mae),\n    }\n\n\ndef _check_outlier_predictions(predictions: np.ndarray, fold_idx: int) -> None:\n    \"\"\"\n    Check for outlier predictions and log warnings.\n\n    Per P3C4-001-006: Log warning if all predictions exceed ±1000 bps (±10.0).\n\n    Args:\n        predictions: Model predictions for a fold\n        fold_idx: Fold index for logging context\n    \"\"\"\n    if len(predictions) == 0:\n        return\n\n    outlier_threshold = 10.0\n    if np.all(np.abs(predictions) > outlier_threshold):\n        logger.warning(\n            f\"Fold {fold_idx}: all predictions exceed ±{outlier_threshold} \"\n            f\"(max={predictions.max():.2f}, min={predictions.min():.2f})\"\n        )\n\n\ndef aggregate_oof_predictions(\n    fold_predictions: Iterable[tuple[Sequence[Any], Sequence[float], int]],\n) -> pd.DataFrame:\n    \"\"\"Aggregate out-of-fold predictions from individual cross-validation folds.\n\n    Args:\n        fold_predictions: Iterable of tuples ``(indices, predictions, fold_id)`` where\n            ``indices`` is a 1-D sequence of sample indices (can be MultiIndex),\n            ``predictions`` is a 1-D sequence aligned with ``indices``,\n            and ``fold_id`` identifies the fold.\n\n    Returns:\n        DataFrame indexed by the provided ``indices`` with columns ``prediction`` and\n        ``fold_id``.\n\n    Raises:\n        ValueError: If no folds are provided, shapes mismatch, indices overlap, or\n            predictions contain NaN/Inf values.\n    \"\"\"\n    entries = list(fold_predictions)\n    if not entries:\n        raise ValueError(\"fold_predictions cannot be empty.\")\n\n    frames: list[pd.DataFrame] = []\n    seen_indices: set[Any] = set()\n\n    for entry in entries:\n        if len(entry) != 3:\n            raise ValueError(\n                \"Each fold prediction entry must be a tuple of (indices, predictions, fold_id).\"\n            )\n\n        indices, predictions, fold_id = entry\n        index = pd.Index(indices)\n        preds_array = np.asarray(predictions, dtype=np.float32)\n\n        if preds_array.ndim != 1:\n            raise ValueError(f\"Predictions for fold {fold_id} must be one-dimensional.\")\n        if len(index) != len(preds_array):\n            raise ValueError(\n                f\"Fold {fold_id} has mismatched indices ({len(index)}) and predictions \"\n                f\"({len(preds_array)}).\"\n            )\n        if not np.isfinite(preds_array).all():\n            raise ValueError(f\"Fold {fold_id} contains NaN or infinite predictions.\")\n\n        index_list = index.tolist()\n        duplicate_indices = set(index_list) & seen_indices\n        if duplicate_indices:\n            duplicates_preview = list(duplicate_indices)[:3]\n            raise ValueError(\n                f\"Detected overlapping OOF indices between folds: {duplicates_preview}\"\n            )\n\n        seen_indices.update(index_list)\n\n        fold_frame = pd.DataFrame({\"prediction\": preds_array}, index=index)\n        fold_frame[\"fold_id\"] = np.int8(fold_id)\n        frames.append(fold_frame)\n\n        logger.debug(\n            \"Aggregated %s predictions for fold %s.\",\n            len(fold_frame),\n            fold_id,\n        )\n\n    aggregated = pd.concat(frames).sort_index()\n    aggregated = aggregated[[\"prediction\", \"fold_id\"]]\n    return aggregated\n\n\n# ============================================================================\n# Multi-Horizon Training Wrapper (P3C4-001-009)\n# ============================================================================\n\n\ndef train_multi_horizon(\n    features: pd.DataFrame,\n    labels: pd.DataFrame,\n    cv_splitter: Any,\n    config: dict[str, Any],\n) -> dict[tuple[str, str], dict[str, Any]]:\n    \"\"\"Train all base models for both 21d and 63d horizons.\n\n    Per P3C4-001-009: Orchestrate training across all model-horizon pairs,\n    handling edge cases (missing labels, index mismatches, partial failures).\n\n    Args:\n        features: Feature matrix with MultiIndex (instrument, datetime)\n        labels: Labels DataFrame with columns [label_21d, label_63d]\n        cv_splitter: CPCV splitter instance\n        config: Configuration dict with keys [models, horizons, ...]\n\n    Returns:\n        Dict mapping (model_name, horizon) -> {\n            'oof': DataFrame,\n            'model': BaseModelTrainer,\n            'cv_scores': List[dict]\n        }\n\n    Raises:\n        KeyError: If required label columns are missing\n        ValueError: If feature-label index mismatch or all models fail\n        RuntimeError: If all model-horizon pairs fail training\n\n    Edge cases:\n        - Label column missing: Raise KeyError with expected columns\n        - Feature-label index mismatch: Inner join, log dropped count\n        - Horizon fails mid-training: Log error, continue with other horizons\n        - All models fail: Raise RuntimeError with summary\n\n    Example:\n        >>> results = train_multi_horizon(features, labels, cv_splitter, config)\n        >>> ridge_21d_oof = results[('ridge', '21d')]['oof']\n        >>> xgb_63d_model = results[('xgboost', '63d')]['model']\n    \"\"\"\n    # TODO P3C4-001-009: Validate label columns exist\n    # Expected columns: ['label_21d', 'label_63d']\n    # Raise KeyError if missing: \"Missing required label columns: {missing}\"\n\n    # TODO P3C4-001-009: Validate features and labels have compatible indices\n    # Use inner join to align features and labels\n    # Log warning if any rows are dropped: \"Dropped {n} rows due to index mismatch\"\n\n    # TODO P3C4-001-009: Extract model names from config['models']\n    # Expected format: config['models'] = ['ridge', 'xgboost']\n    # Default to ['ridge', 'xgboost'] if not present\n\n    # TODO P3C4-001-009: Extract horizons from config or labels\n    # Default to ['21d', '63d'] based on label column naming\n\n    # TODO P3C4-001-009: Iterate over all (model_name, horizon) pairs\n    # For each pair:\n    #   1. Get trainer instance from model_registry.get_model(model_name)\n    #   2. Extract label series for horizon: y = labels[f'label_{horizon}']\n    #   3. Call run_cv_training(features, y, cv_splitter, trainer, model_name, horizon)\n    #   4. Store result in results dict: results[(model_name, horizon)] = cv_result\n    #   5. Handle exceptions: log error, continue if config allows partial failures\n\n    # TODO P3C4-001-009: After all training, validate at least one pair succeeded\n    # If all pairs failed, raise RuntimeError with summary of failures\n\n    # TODO P3C4-001-009: Log summary of successful/failed pairs\n    # Format: \"Multi-horizon training complete: {n_success}/{n_total} pairs successful\"\n\n    raise NotImplementedError(\n        \"P3C4-001-009: train_multi_horizon stub - implementation required\"\n    )\n\n\ndef save_multi_horizon_results(\n    results: dict[tuple[str, str], dict[str, Any]],\n    output_dir: Path,\n    region: str,\n) -> None:\n    \"\"\"Save all multi-horizon training outputs to disk.\n\n    Per P3C4-001-009: Persist OOF predictions, models, CV scores, and feature\n    importance for all model-horizon pairs.\n\n    Args:\n        results: Output from train_multi_horizon()\n        output_dir: Base directory for outputs (e.g., data/model2/us/)\n        region: Region identifier (\"US\" or \"CN\")\n\n    Raises:\n        OSError: If any file write operations fail\n        ValueError: If results dict is empty\n\n    Edge cases:\n        - Empty results: Raise ValueError\n        - Output directory doesn't exist: Create with parents\n        - Partial save failure: Log error, continue with remaining outputs\n\n    Output structure:\n        {output_dir}/\n            oof/\n                ridge_21d_oof.parquet\n                ridge_63d_oof.parquet\n                xgboost_21d_oof.parquet\n                xgboost_63d_oof.parquet\n            models/\n                ridge_21d.pkl\n                ridge_63d.pkl\n                xgboost_21d.pkl\n                xgboost_63d.pkl\n            cv_scores/\n                ridge_21d_cv_scores.json\n                ridge_63d_cv_scores.json\n                xgboost_21d_cv_scores.json\n                xgboost_63d_cv_scores.json\n            feature_importance/\n                xgboost_21d_importance.parquet\n                xgboost_63d_importance.parquet\n    \"\"\"\n    # TODO P3C4-001-009: Validate results dict is not empty\n    # Raise ValueError if empty: \"Cannot save empty results\"\n\n    # TODO P3C4-001-009: Create output subdirectories\n    # Subdirs: oof/, models/, cv_scores/, feature_importance/\n\n    # TODO P3C4-001-009: Iterate over all (model_name, horizon) pairs\n    # For each pair:\n    #   1. Save OOF predictions using persistence.save_oof_predictions()\n    #      Path: {output_dir}/oof/{model_name}_{horizon}_oof.parquet\n    #   2. Save final model using persistence.save_trained_model()\n    #      Path: {output_dir}/models/{model_name}_{horizon}.pkl\n    #   3. Save CV scores to JSON\n    #      Path: {output_dir}/cv_scores/{model_name}_{horizon}_cv_scores.json\n    #   4. If model has feature importance (XGBoost), save using save_feature_importance()\n    #      Path: {output_dir}/feature_importance/{model_name}_{horizon}_importance.parquet\n\n    # TODO P3C4-001-009: Log summary of saved files\n    # Format: \"Saved outputs for {n} model-horizon pairs to {output_dir}\"\n\n    raise NotImplementedError(\n        \"P3C4-001-009: save_multi_horizon_results stub - implementation required\"\n    )\n"
    }
  ],
  "description": "Add stubs for P3C4-001-009 multi-horizon training wrapper. Provides function signatures, docstrings, and TODO markers for train_multi_horizon() and save_multi_horizon_results(). Preserves all existing implementations (RidgeTrainer, XGBoostTrainer, run_cv_training, aggregate_oof_predictions) from dependencies P3C4-001-006, 007, 008.",
  "context": "Dependencies P3C4-001-006 (CV training orchestrator), P3C4-001-007 (feature importance), and P3C4-001-008 (model persistence) are complete. This ticket adds the top-level wrapper that trains all (model, horizon) pairs: (ridge, 21d), (ridge, 63d), (xgboost, 21d), (xgboost, 63d). The wrapper handles label validation, feature-label joins, partial failures, and batch saving of all outputs.",
  "notes": "Implementation approach: (1) Validate label columns [label_21d, label_63d] exist, (2) Inner join features and labels on MultiIndex, (3) Retrieve trainers from model_registry, (4) Loop over all 4 (model, horizon) pairs calling run_cv_training(), (5) Collect results into dict[(model, horizon)] -> {oof, model, cv_scores}, (6) Save all outputs using persistence utilities. Edge cases: missing labels -> KeyError, index mismatch -> inner join + warning, partial failures -> continue if config allows, all failures -> RuntimeError. Tests use pytest.skip() to avoid breaking test suite until implementation is ready.",
  "tests": [
    {
      "path": "tests/unit/test_model2_multi_horizon.py",
      "content": "\"\"\"Unit tests for multi-horizon training wrapper (P3C4-001-009).\n\nTests:\n- test_multi_horizon_training_success: All 4 model-horizon pairs train successfully\n- test_multi_horizon_feature_label_join: Inner join on indices, log dropped rows\n- test_multi_horizon_partial_failure: Continue on single model failure (if configured)\n- test_multi_horizon_all_outputs: Verify all 4 OOF files and 4 model files saved\n- test_multi_horizon_missing_label: Raise KeyError for missing label column\n- test_multi_horizon_all_failures: Raise RuntimeError if all pairs fail\n\nAll tests skipped until P3C4-001-009 implementation is complete.\n\"\"\"\n\nimport pytest\n\npytestmark = pytest.mark.skip(reason=\"P3C4-001-009: Multi-horizon wrapper not yet implemented\")\n\n\nclass TestMultiHorizonTraining:\n    \"\"\"Test suite for train_multi_horizon() function.\"\"\"\n\n    def test_multi_horizon_training_success(self):\n        \"\"\"Test all 4 model-horizon pairs train successfully.\n\n        Acceptance:\n        - Results dict has keys: (ridge, 21d), (ridge, 63d), (xgboost, 21d), (xgboost, 63d)\n        - Each result has keys: ['oof', 'model', 'cv_scores']\n        - OOF predictions are DataFrame with columns [prediction, fold_id]\n        - Model is fitted BaseModelTrainer instance\n        - CV scores is list of dicts with keys [model, horizon, fold_id, r2, mse, mae]\n        \"\"\"\n        pass\n\n    def test_multi_horizon_feature_label_join(self):\n        \"\"\"Test inner join on features and labels with index mismatch.\n\n        Acceptance:\n        - Features have 500 rows, labels have 480 rows (20 missing)\n        - Inner join produces 480 aligned samples\n        - Warning logged: 'Dropped 20 rows due to index mismatch'\n        - Training proceeds on 480 samples\n        \"\"\"\n        pass\n\n    def test_multi_horizon_partial_failure(self):\n        \"\"\"Test continues on single model failure if config allows.\n\n        Acceptance:\n        - Mock XGBoostTrainer to raise ValueError during fit\n        - Ridge models train successfully for both horizons\n        - XGBoost failures logged: 'Model xgboost horizon 21d training failed'\n        - Results dict has only (ridge, 21d) and (ridge, 63d)\n        - No RuntimeError raised (partial success allowed)\n        \"\"\"\n        pass\n\n    def test_multi_horizon_all_outputs(self):\n        \"\"\"Test all outputs saved to correct directories.\n\n        Acceptance:\n        - 4 OOF parquet files in {output_dir}/oof/\n        - 4 model pickle files in {output_dir}/models/\n        - 4 CV score JSON files in {output_dir}/cv_scores/\n        - 2 feature importance parquet files in {output_dir}/feature_importance/ (XGBoost only)\n        - All files named correctly: {model}_{horizon}_*.{ext}\n        \"\"\"\n        pass\n\n    def test_multi_horizon_missing_label(self):\n        \"\"\"Test raises KeyError for missing label column.\n\n        Acceptance:\n        - Labels DataFrame has only [label_21d] (missing label_63d)\n        - train_multi_horizon() raises KeyError\n        - Error message: 'Missing required label columns: {label_63d}'\n        \"\"\"\n        pass\n\n    def test_multi_horizon_all_failures(self):\n        \"\"\"Test raises RuntimeError if all model-horizon pairs fail.\n\n        Acceptance:\n        - Mock all trainers to raise ValueError during fit\n        - train_multi_horizon() raises RuntimeError\n        - Error message contains failure summary: 'All 4 model-horizon pairs failed'\n        \"\"\"\n        pass\n\n\nclass TestSaveMultiHorizonResults:\n    \"\"\"Test suite for save_multi_horizon_results() function.\"\"\"\n\n    def test_save_all_outputs(self):\n        \"\"\"Test all outputs saved to correct locations.\n\n        Acceptance:\n        - All subdirectories created: oof/, models/, cv_scores/, feature_importance/\n        - All files saved with correct naming: {model}_{horizon}_*.{ext}\n        - File counts: 4 OOF, 4 models, 4 CV scores, 2 feature importance\n        \"\"\"\n        pass\n\n    def test_save_empty_results(self):\n        \"\"\"Test raises ValueError for empty results dict.\n\n        Acceptance:\n        - Empty results dict raises ValueError\n        - Error message: 'Cannot save empty results'\n        \"\"\"\n        pass\n\n    def test_save_creates_directories(self):\n        \"\"\"Test creates output directories if they don't exist.\n\n        Acceptance:\n        - Output directory does not exist initially\n        - save_multi_horizon_results() creates all subdirectories\n        - All files saved successfully\n        \"\"\"\n        pass\n"
    }
  ]
}
