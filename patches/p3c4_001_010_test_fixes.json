{
  "ticket": "P3C4-001-010-FIX-TESTS",
  "description": "Fix 3 test failures: make negative r2 test deterministic, relax constant target assertion, add logger configuration",
  "context": "Per Codex review: (1) test_compute_cv_scores_negative_r2 had conditional assertions that would pass even if code stopped returning negative values - made deterministic with y_pred=-y_true+noise and unconditional assertions; (2) sklearn 1.5+ returns r2=0.0 for constant targets instead of NaN - relaxed assertion; (3) caplog not capturing logs - added manual logger configuration",
  "files": [
    {
      "path": "tests/unit/test_model2_cv_scores.py",
      "content": "\"\"\"Unit tests for CV score logging and aggregation (P3C4-001-010).\n\nTests:\n- test_compute_cv_scores_normal: Verify r2, mse, mae computed correctly on toy data\n- test_compute_cv_scores_constant_target: Verify r2=NaN logged with warning\n- test_compute_cv_scores_nan_predictions: Verify ValueError raised for NaN predictions\n- test_compute_cv_scores_negative_r2: Verify negative r2 logged with warning\n- test_validate_cv_score_schema_valid: Verify valid schema passes\n- test_validate_cv_score_schema_missing_keys: Verify ValueError for missing keys\n- test_validate_cv_score_schema_invalid_types: Verify ValueError for wrong types\n- test_aggregate_cv_scores_normal: Verify mean and std computed across folds\n- test_aggregate_cv_scores_with_nan: Verify np.nanmean/np.nanstd handle NaN\n- test_aggregate_cv_scores_empty: Verify ValueError for empty list\n- test_log_cv_scores_json: Verify JSON output with allow_nan=True\n- test_log_cv_scores_json_invalid_schema: Verify ValueError for schema violation\n- test_outlier_threshold_constant: Verify OUTLIER_THRESHOLD_BPS used instead of hardcoded 5.0\n\"\"\"\n\nimport json\nimport tempfile\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport pytest\n\nfrom src.model2.base_models import (\n    OUTLIER_THRESHOLD_BPS,\n    CV_SCORE_SCHEMA_KEYS,\n    aggregate_cv_scores,\n    compute_cv_scores,\n    log_cv_scores_json,\n    validate_cv_score_schema,\n    _check_outlier_predictions,\n)\n\n\nclass TestComputeCVScores:\n    \"\"\"Test suite for compute_cv_scores() function.\"\"\"\n\n    def test_compute_cv_scores_normal(self):\n        \"\"\"Test CV score computation on normal data.\n\n        Acceptance:\n        - Schema: {model, horizon, fold_id, r2, mse, mae}\n        - All metrics are finite floats\n        - r2 \u2208 [-\u221e, 1], mse \u2265 0, mae \u2265 0\n        \"\"\"\n        np.random.seed(42)\n\n        # Create synthetic data\n        y_true = pd.Series(np.random.randn(100))\n        y_pred = y_true + np.random.randn(100) * 0.1  # Add small noise\n\n        # Compute CV scores\n        scores = compute_cv_scores(\n            y_true=y_true,\n            y_pred=y_pred,\n            fold_idx=0,\n            model_name=\"ridge\",\n            horizon=\"21d\",\n        )\n\n        # Verify schema\n        assert set(scores.keys()) == CV_SCORE_SCHEMA_KEYS\n        assert scores[\"model\"] == \"ridge\"\n        assert scores[\"horizon\"] == \"21d\"\n        assert scores[\"fold_id\"] == 0\n\n        # Verify metrics are finite\n        assert np.isfinite(scores[\"r2\"])\n        assert np.isfinite(scores[\"mse\"])\n        assert np.isfinite(scores[\"mae\"])\n\n        # Verify metric bounds\n        assert scores[\"r2\"] <= 1.0  # r2 can be negative but max is 1.0\n        assert scores[\"mse\"] >= 0.0\n        assert scores[\"mae\"] >= 0.0\n\n        # With low noise, r2 should be high\n        assert scores[\"r2\"] > 0.8\n\n    def test_compute_cv_scores_constant_target(self, caplog):\n        \"\"\"Test CV score computation with constant target.\n\n        Acceptance:\n        - r2 is 0.0 or NaN (sklearn may return either for constant target)\n        - Warning logged: 'constant target detected'\n        - mse and mae are finite\n        \"\"\"\n        import logging\n\n        # Create constant target\n        np.random.seed(42)\n        y_true = pd.Series(np.ones(100) * 5.0)\n        y_pred = np.random.randn(100)  # Random predictions\n\n        # Configure logger to capture warnings (pattern from test_model2_base_trainers.py)\n        test_logger = logging.getLogger(\"src.model2.base_models\")\n        previous_level = test_logger.level\n        previous_propagate = test_logger.propagate\n        test_logger.setLevel(logging.WARNING)\n        test_logger.addHandler(caplog.handler)\n        test_logger.propagate = False\n        caplog.set_level(logging.WARNING)\n\n        try:\n            with caplog.at_level(logging.WARNING, logger=\"src.model2.base_models\"):\n                scores = compute_cv_scores(\n                    y_true=y_true,\n                    y_pred=y_pred,\n                    fold_idx=0,\n                    model_name=\"ridge\",\n                    horizon=\"21d\",\n                )\n        finally:\n            test_logger.propagate = previous_propagate\n            test_logger.removeHandler(caplog.handler)\n            test_logger.setLevel(previous_level)\n\n        # Verify r2 is 0.0 or NaN (sklearn behavior varies by version)\n        assert scores[\"r2\"] == 0.0 or np.isnan(scores[\"r2\"])\n\n        # Verify mse and mae are finite\n        assert np.isfinite(scores[\"mse\"])\n        assert np.isfinite(scores[\"mae\"])\n\n        # Verify warning logged\n        assert \"constant target detected\" in caplog.text\n\n    def test_compute_cv_scores_nan_predictions(self):\n        \"\"\"Test CV score computation raises ValueError for NaN predictions.\n\n        Acceptance:\n        - ValueError raised before metric computation\n        - Error message: 'predictions contain NaN or Inf values'\n        \"\"\"\n        y_true = pd.Series(np.random.randn(100))\n        y_pred = np.random.randn(100)\n        y_pred[50] = np.nan  # Inject NaN\n\n        # Should raise ValueError\n        with pytest.raises(ValueError, match=\"predictions contain NaN or Inf\"):\n            compute_cv_scores(\n                y_true=y_true,\n                y_pred=y_pred,\n                fold_idx=0,\n                model_name=\"ridge\",\n                horizon=\"21d\",\n            )\n\n    def test_compute_cv_scores_negative_r2(self, caplog):\n        \"\"\"Test CV score computation logs warning for negative r2.\n\n        Acceptance:\n        - r2 < 0 (model worse than mean baseline) - DETERMINISTIC\n        - Warning logged: 'negative r2'\n        - r2 value is preserved (not clipped to 0)\n        \"\"\"\n        import logging\n\n        # Create data that GUARANTEES negative r2 (deterministic per Codex review)\n        np.random.seed(42)\n        y_true = pd.Series(np.random.randn(100))\n        # Predictions are negatively correlated with true values\n        y_pred = -y_true.values + np.random.randn(100) * 0.1\n\n        # Configure logger to capture warnings\n        test_logger = logging.getLogger(\"src.model2.base_models\")\n        previous_level = test_logger.level\n        previous_propagate = test_logger.propagate\n        test_logger.setLevel(logging.WARNING)\n        test_logger.addHandler(caplog.handler)\n        test_logger.propagate = False\n        caplog.set_level(logging.WARNING)\n\n        try:\n            with caplog.at_level(logging.WARNING, logger=\"src.model2.base_models\"):\n                scores = compute_cv_scores(\n                    y_true=y_true,\n                    y_pred=y_pred,\n                    fold_idx=0,\n                    model_name=\"ridge\",\n                    horizon=\"21d\",\n                )\n        finally:\n            test_logger.propagate = previous_propagate\n            test_logger.removeHandler(caplog.handler)\n            test_logger.setLevel(previous_level)\n\n        # Verify r2 is negative (UNCONDITIONAL - per Codex review)\n        assert scores[\"r2\"] < 0, f\"Expected negative r2, got {scores['r2']}\"\n\n        # Verify warning logged (UNCONDITIONAL)\n        assert \"negative r2\" in caplog.text\n\n        # Verify value is preserved (not clipped)\n        assert np.isfinite(scores[\"r2\"])\n\n\nclass TestValidateCVScoreSchema:\n    \"\"\"Test suite for validate_cv_score_schema() function.\"\"\"\n\n    def test_validate_cv_score_schema_valid(self):\n        \"\"\"Test schema validation passes for valid CV score dict.\"\"\"\n        valid_score = {\n            \"model\": \"ridge\",\n            \"horizon\": \"21d\",\n            \"fold_id\": 0,\n            \"r2\": 0.85,\n            \"mse\": 0.12,\n            \"mae\": 0.28,\n        }\n\n        # Should not raise\n        validate_cv_score_schema(valid_score)\n\n    def test_validate_cv_score_schema_missing_keys(self):\n        \"\"\"Test schema validation raises ValueError for missing keys.\"\"\"\n        incomplete_score = {\n            \"model\": \"ridge\",\n            \"horizon\": \"21d\",\n            # Missing: fold_id, r2, mse, mae\n        }\n\n        with pytest.raises(ValueError, match=\"missing required keys\"):\n            validate_cv_score_schema(incomplete_score)\n\n    def test_validate_cv_score_schema_invalid_types(self):\n        \"\"\"Test schema validation raises ValueError for wrong types.\"\"\"\n        # Test invalid model type (should be str)\n        invalid_model_type = {\n            \"model\": 123,  # Should be str\n            \"horizon\": \"21d\",\n            \"fold_id\": 0,\n            \"r2\": 0.85,\n            \"mse\": 0.12,\n            \"mae\": 0.28,\n        }\n        with pytest.raises(ValueError, match=\"'model' must be str\"):\n            validate_cv_score_schema(invalid_model_type)\n\n        # Test invalid fold_id type (should be int)\n        invalid_fold_id_type = {\n            \"model\": \"ridge\",\n            \"horizon\": \"21d\",\n            \"fold_id\": \"0\",  # Should be int\n            \"r2\": 0.85,\n            \"mse\": 0.12,\n            \"mae\": 0.28,\n        }\n        with pytest.raises(ValueError, match=\"'fold_id' must be int\"):\n            validate_cv_score_schema(invalid_fold_id_type)\n\n\nclass TestAggregateCVScores:\n    \"\"\"Test suite for aggregate_cv_scores() function.\"\"\"\n\n    def test_aggregate_cv_scores_normal(self):\n        \"\"\"Test CV score aggregation on normal data.\n\n        Acceptance:\n        - Aggregate schema: {n_folds, r2_mean, r2_std, mse_mean, mse_std, mae_mean, mae_std}\n        - Mean and std computed correctly\n        \"\"\"\n        cv_scores = [\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 0, \"r2\": 0.8, \"mse\": 0.1, \"mae\": 0.2},\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 1, \"r2\": 0.85, \"mse\": 0.09, \"mae\": 0.19},\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 2, \"r2\": 0.9, \"mse\": 0.08, \"mae\": 0.18},\n        ]\n\n        agg_scores = aggregate_cv_scores(cv_scores)\n\n        # Verify schema\n        assert \"n_folds\" in agg_scores\n        assert \"r2_mean\" in agg_scores\n        assert \"r2_std\" in agg_scores\n        assert \"mse_mean\" in agg_scores\n        assert \"mse_std\" in agg_scores\n        assert \"mae_mean\" in agg_scores\n        assert \"mae_std\" in agg_scores\n\n        # Verify values\n        assert agg_scores[\"n_folds\"] == 3\n        assert np.isclose(agg_scores[\"r2_mean\"], 0.85)\n        assert np.isclose(agg_scores[\"mse_mean\"], 0.09)\n        assert np.isclose(agg_scores[\"mae_mean\"], 0.19)\n\n        # Verify std is positive\n        assert agg_scores[\"r2_std\"] > 0\n        assert agg_scores[\"mse_std\"] > 0\n        assert agg_scores[\"mae_std\"] > 0\n\n    def test_aggregate_cv_scores_with_nan(self):\n        \"\"\"Test CV score aggregation handles NaN values.\n\n        Acceptance:\n        - Uses np.nanmean/np.nanstd to ignore NaN\n        - Aggregate computed from non-NaN values\n        \"\"\"\n        cv_scores = [\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 0, \"r2\": np.nan, \"mse\": 0.1, \"mae\": 0.2},\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 1, \"r2\": 0.85, \"mse\": 0.09, \"mae\": 0.19},\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 2, \"r2\": 0.9, \"mse\": 0.08, \"mae\": 0.18},\n        ]\n\n        agg_scores = aggregate_cv_scores(cv_scores)\n\n        # Verify r2_mean computed from non-NaN values (0.85, 0.9)\n        assert agg_scores[\"n_folds\"] == 3\n        assert np.isclose(agg_scores[\"r2_mean\"], 0.875)  # Mean of 0.85 and 0.9\n\n    def test_aggregate_cv_scores_empty(self):\n        \"\"\"Test CV score aggregation raises ValueError for empty list.\"\"\"\n        with pytest.raises(ValueError, match=\"Cannot aggregate empty cv_scores\"):\n            aggregate_cv_scores([])\n\n\nclass TestLogCVScoresJSON:\n    \"\"\"Test suite for log_cv_scores_json() function.\"\"\"\n\n    def test_log_cv_scores_json(self):\n        \"\"\"Test CV scores written to JSON with allow_nan=True.\n\n        Acceptance:\n        - JSON file created at output_path\n        - Contains all CV scores\n        - allow_nan=True enables NaN serialization\n        \"\"\"\n        cv_scores = [\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 0, \"r2\": 0.8, \"mse\": 0.1, \"mae\": 0.2},\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 1, \"r2\": np.nan, \"mse\": 0.09, \"mae\": 0.19},\n        ]\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            output_path = Path(tmpdir) / \"cv_scores\" / \"test_cv_scores.json\"\n\n            # Write CV scores\n            log_cv_scores_json(cv_scores, output_path)\n\n            # Verify file exists\n            assert output_path.exists()\n\n            # Load and verify contents\n            with open(output_path) as f:\n                loaded_scores = json.load(f)\n\n            assert len(loaded_scores) == 2\n            assert loaded_scores[0][\"fold_id\"] == 0\n            assert loaded_scores[0][\"r2\"] == 0.8\n\n            # NaN should be serialized as NaN (not null)\n            # In JSON, NaN is represented as NaN (not a standard JSON value but allowed with allow_nan=True)\n            assert np.isnan(loaded_scores[1][\"r2\"])\n\n    def test_log_cv_scores_json_invalid_schema(self):\n        \"\"\"Test log_cv_scores_json raises ValueError for schema violation.\"\"\"\n        invalid_scores = [\n            {\"model\": \"ridge\", \"horizon\": \"21d\", \"fold_id\": 0},  # Missing r2, mse, mae\n        ]\n\n        with tempfile.TemporaryDirectory() as tmpdir:\n            output_path = Path(tmpdir) / \"cv_scores.json\"\n\n            with pytest.raises(ValueError, match=\"missing required keys\"):\n                log_cv_scores_json(invalid_scores, output_path)\n\n    def test_log_cv_scores_json_empty(self):\n        \"\"\"Test log_cv_scores_json raises ValueError for empty list.\"\"\"\n        with tempfile.TemporaryDirectory() as tmpdir:\n            output_path = Path(tmpdir) / \"cv_scores.json\"\n\n            with pytest.raises(ValueError, match=\"Cannot write empty cv_scores\"):\n                log_cv_scores_json([], output_path)\n\n\nclass TestOutlierThresholdConstant:\n    \"\"\"Test suite for OUTLIER_THRESHOLD_BPS constant.\"\"\"\n\n    def test_outlier_threshold_constant_value(self):\n        \"\"\"Test OUTLIER_THRESHOLD_BPS has correct value.\n\n        Acceptance:\n        - OUTLIER_THRESHOLD_BPS = 5.0 (\u00b1500 bps)\n        \"\"\"\n        assert OUTLIER_THRESHOLD_BPS == 5.0\n\n    def test_outlier_threshold_used_in_check(self, caplog):\n        \"\"\"Test _check_outlier_predictions uses OUTLIER_THRESHOLD_BPS.\n\n        Acceptance:\n        - Warning logged when predictions exceed \u00b1OUTLIER_THRESHOLD_BPS\n        - Warning message includes threshold value\n        \"\"\"\n        import logging\n\n        # Create predictions that exceed threshold\n        predictions = np.array([6.0, 7.0, -6.0, -7.0] * 10)  # 40 predictions, all exceed \u00b15.0\n\n        # Configure logger to capture warnings\n        test_logger = logging.getLogger(\"src.model2.base_models\")\n        previous_level = test_logger.level\n        previous_propagate = test_logger.propagate\n        test_logger.setLevel(logging.WARNING)\n        test_logger.addHandler(caplog.handler)\n        test_logger.propagate = False\n        caplog.set_level(logging.WARNING)\n\n        try:\n            with caplog.at_level(logging.WARNING, logger=\"src.model2.base_models\"):\n                _check_outlier_predictions(predictions, fold_idx=0)\n        finally:\n            test_logger.propagate = previous_propagate\n            test_logger.removeHandler(caplog.handler)\n            test_logger.setLevel(previous_level)\n\n        # Verify warning logged with threshold value\n        assert \"all predictions exceed \u00b15.0\" in caplog.text or \"100.00% of predictions exceed \u00b15.0\" in caplog.text\n"
    }
  ]
}